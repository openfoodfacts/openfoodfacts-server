{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Product Opener (Open Food Facts web server) documentation # Welcome to the documentation of Product Opener , the web server at the heart of Open Food Facts project. It also powers the sibling Open [ Beauty | Pet Food | Products ] Facts projects If you want to use the API or are interested in the data, look at API documentation If you are a developer, look at Developer documentation The repository for the project is at https://github.com/openfoodfacts/openfoodfacts-server/ Do not hesitate to contribute to this documentation, this is highly appreciated.","title":"Product Opener (Open Food Facts web server) documentation"},{"location":"#product-opener-open-food-facts-web-server-documentation","text":"Welcome to the documentation of Product Opener , the web server at the heart of Open Food Facts project. It also powers the sibling Open [ Beauty | Pet Food | Products ] Facts projects If you want to use the API or are interested in the data, look at API documentation If you are a developer, look at Developer documentation The repository for the project is at https://github.com/openfoodfacts/openfoodfacts-server/ Do not hesitate to contribute to this documentation, this is highly appreciated.","title":"Product Opener (Open Food Facts web server) documentation"},{"location":"api/","text":"Introduction to Open Food Facts API documentation # Everything you need to know about Open Food Facts API. Overview # Open Food Facts is a food products database made by everyone, for everyone, that can help you make better choices about what you eat. Being open data, anyone can reuse it for any purpose. The Open Food Facts API enables developers to get information like ingredients and nutritional values of products, and even add more facts to the products database. You may use the API to build applications that allow users to contribute to the database and make healthier food choices. The current version of the API is 2 . Data in the Open Food Facts database is provided voluntarily by users who want to support the program. As a result, there are no assurances that the data is accurate, complete, or reliable. The user assumes the entire risk of using the data. Before You Start # The Open Food Facts database is available under the Open Database License . The individual contents of the database are available under the Database Contents License . Product images are available under the Creative Commons Attribution ShareAlike license. They may contain graphical elements subject to copyright or other rights that may, in some cases, be reproduced (quotation rights or fair use). Please read the Terms and conditions of use and reuse before reusing the data. We want to learn what the Open Food Facts data is used for. It is not mandatory, but we would appreciate it if you tell us about your use-case so that we can share them with the Open Food Facts community. How to Best Use the API # General principles # You can search for product information, including many useful computed values. Suppose we don't have the information you need on a specific product. In that case, you (or your users) can upload the product photos, and the backend (and our AI algorithms!) will process them, generating helpful info. The photos will also be available for the users of OpenFoodFacts and every other API user. You could also ask your user to enter some of the information about the product (like name, category, and weight) so that they immediately get the computed info. Generally, the more information we have about a product, the more we can compute it. If your users do not expect a result immediately (e.g., Inventory apps) # Submit photos (front packaging/nutrition values/ingredients): the most painless thing for your users. The backend (Product Opener) and Open Food Facts AI (Robotoff) will generate some derived data from the photos. Over time, other apps and the Open Food Facts community will fill the data gaps. If your users expect a result immediately (e.g., Nutrition apps) # If you submit the product's nutritional values and category , you'll get the Nutri-Score . If you submit the product ingredients , you'll get the NOVA group (about food ultra-processing), additives , allergens , normalized ingredients , vegan , vegetarian \u2026 If you submit the product's category and labels , you'll (soon) get the Eco-Score (a rating of the product environmental impact) API Deployments # The OpenFoodFacts API has two deployments. Production: https://world.openfoodfacts.org Staging: https://world.openfoodfacts.net Consider using the staging environment if you are not in a production scenario. While testing your applications, make all API requests to the staging environment . This way, we can ensure the product database is safe. Warning : The staging environment has an extra level of HTTP Basic Authentication (username: off , password: off ). When making API requests to staging, you may use https://off:off@world.openfoodfacts.net/ as the base URL to include the authentication. Authentication # READ operations (getting info about a product, etc...) do not require authentication, although we recommend using a custom User-Agent if you're developing an application (to not risk being identified as a bot) WRITE operations (Editing an Existing Product, Uploading images\u2026) require authentication . We do this as another layer of protection against spam. Create an account on the Open Food Facts app . From there, you then have two alternatives: The preferred one : Use the login API to get a session cookie and use this cookie in your subsequent request to be authenticated. However, the session must always be used from the same IP address, and you have a maximum of sessions per user. If session conditions are too restrictive for your use case, include your account credentials as parameters for authenticated requests where user_id is your username and password is your password (do this on POST / PUT / DELETE request, not on GET) You can create a global account to allow your app users to contribute without registering individual accounts on the Open Food Facts website. This way, we know that these contributions came from your application. Production and staging have different account databases, so the account you create in the production environment will only work for production requests . If you want to query (WRITE requests) the staging environment, you'll need to create another account there too. Reference Documentation (OpenAPI) # We are building a complete OpenAPI reference. Here is a list of the current API documentation available: OpenAPI documentation (v2) OpenAPI documentation for v3 (for packaging components only) A cheatsheet to remind some usual patterns. Tutorials # A comprehensive introduction to Using the Open Food Facts API . Uploading images to the Open Food Facts API Help # Try the FAQ - to answer most of your questions. Didn't get what you wanted? Contact the Team on the #api Slack Channel . Report Bugs on the Open Food Facts GitHub repository. Do you have an issue or feature request? You can submit it on GitHub too. Are you interested in contributing to this project? See our Contribution Guidelines . SDKs # SDKs are available for specific languages to facilitate the usage of the API. We may have a wrapper for your favourite programming language. If we do, you can use it and improve it. However, If we don't, you can help us create it! They will let you consume data and let your users contribute new data. Open-source contributors develop our SDKs, and more contributions are welcome to improve these SDKs. You can start by checking the existing issues in their respective repositories. Warning : Before exploring any SDK, please read the Before You Start section . Also, remember to check the API Reference Documentation first to verify if the problem is in SDK implementation or in the API itself. Cordova Dart , published on pub.dev Elixir Go NodeJS PHP Laravel Python , published on pypi React Native Ruby Java Rust R","title":"Introduction to Open Food Facts API documentation"},{"location":"api/#introduction-to-open-food-facts-api-documentation","text":"Everything you need to know about Open Food Facts API.","title":"Introduction to Open Food Facts API documentation"},{"location":"api/#overview","text":"Open Food Facts is a food products database made by everyone, for everyone, that can help you make better choices about what you eat. Being open data, anyone can reuse it for any purpose. The Open Food Facts API enables developers to get information like ingredients and nutritional values of products, and even add more facts to the products database. You may use the API to build applications that allow users to contribute to the database and make healthier food choices. The current version of the API is 2 . Data in the Open Food Facts database is provided voluntarily by users who want to support the program. As a result, there are no assurances that the data is accurate, complete, or reliable. The user assumes the entire risk of using the data.","title":"Overview"},{"location":"api/#before-you-start","text":"The Open Food Facts database is available under the Open Database License . The individual contents of the database are available under the Database Contents License . Product images are available under the Creative Commons Attribution ShareAlike license. They may contain graphical elements subject to copyright or other rights that may, in some cases, be reproduced (quotation rights or fair use). Please read the Terms and conditions of use and reuse before reusing the data. We want to learn what the Open Food Facts data is used for. It is not mandatory, but we would appreciate it if you tell us about your use-case so that we can share them with the Open Food Facts community.","title":"Before You Start"},{"location":"api/#how-to-best-use-the-api","text":"","title":"How to Best Use the API"},{"location":"api/#general-principles","text":"You can search for product information, including many useful computed values. Suppose we don't have the information you need on a specific product. In that case, you (or your users) can upload the product photos, and the backend (and our AI algorithms!) will process them, generating helpful info. The photos will also be available for the users of OpenFoodFacts and every other API user. You could also ask your user to enter some of the information about the product (like name, category, and weight) so that they immediately get the computed info. Generally, the more information we have about a product, the more we can compute it.","title":"General principles"},{"location":"api/#if-your-users-do-not-expect-a-result-immediately-eg-inventory-apps","text":"Submit photos (front packaging/nutrition values/ingredients): the most painless thing for your users. The backend (Product Opener) and Open Food Facts AI (Robotoff) will generate some derived data from the photos. Over time, other apps and the Open Food Facts community will fill the data gaps.","title":"If your users do not expect a result immediately (e.g., Inventory apps)"},{"location":"api/#if-your-users-expect-a-result-immediately-eg-nutrition-apps","text":"If you submit the product's nutritional values and category , you'll get the Nutri-Score . If you submit the product ingredients , you'll get the NOVA group (about food ultra-processing), additives , allergens , normalized ingredients , vegan , vegetarian \u2026 If you submit the product's category and labels , you'll (soon) get the Eco-Score (a rating of the product environmental impact)","title":"If your users expect a result immediately (e.g., Nutrition apps)"},{"location":"api/#api-deployments","text":"The OpenFoodFacts API has two deployments. Production: https://world.openfoodfacts.org Staging: https://world.openfoodfacts.net Consider using the staging environment if you are not in a production scenario. While testing your applications, make all API requests to the staging environment . This way, we can ensure the product database is safe. Warning : The staging environment has an extra level of HTTP Basic Authentication (username: off , password: off ). When making API requests to staging, you may use https://off:off@world.openfoodfacts.net/ as the base URL to include the authentication.","title":"API Deployments"},{"location":"api/#authentication","text":"READ operations (getting info about a product, etc...) do not require authentication, although we recommend using a custom User-Agent if you're developing an application (to not risk being identified as a bot) WRITE operations (Editing an Existing Product, Uploading images\u2026) require authentication . We do this as another layer of protection against spam. Create an account on the Open Food Facts app . From there, you then have two alternatives: The preferred one : Use the login API to get a session cookie and use this cookie in your subsequent request to be authenticated. However, the session must always be used from the same IP address, and you have a maximum of sessions per user. If session conditions are too restrictive for your use case, include your account credentials as parameters for authenticated requests where user_id is your username and password is your password (do this on POST / PUT / DELETE request, not on GET) You can create a global account to allow your app users to contribute without registering individual accounts on the Open Food Facts website. This way, we know that these contributions came from your application. Production and staging have different account databases, so the account you create in the production environment will only work for production requests . If you want to query (WRITE requests) the staging environment, you'll need to create another account there too.","title":"Authentication"},{"location":"api/#reference-documentation-openapi","text":"We are building a complete OpenAPI reference. Here is a list of the current API documentation available: OpenAPI documentation (v2) OpenAPI documentation for v3 (for packaging components only) A cheatsheet to remind some usual patterns.","title":"Reference Documentation (OpenAPI)"},{"location":"api/#tutorials","text":"A comprehensive introduction to Using the Open Food Facts API . Uploading images to the Open Food Facts API","title":"Tutorials"},{"location":"api/#help","text":"Try the FAQ - to answer most of your questions. Didn't get what you wanted? Contact the Team on the #api Slack Channel . Report Bugs on the Open Food Facts GitHub repository. Do you have an issue or feature request? You can submit it on GitHub too. Are you interested in contributing to this project? See our Contribution Guidelines .","title":"Help"},{"location":"api/#sdks","text":"SDKs are available for specific languages to facilitate the usage of the API. We may have a wrapper for your favourite programming language. If we do, you can use it and improve it. However, If we don't, you can help us create it! They will let you consume data and let your users contribute new data. Open-source contributors develop our SDKs, and more contributions are welcome to improve these SDKs. You can start by checking the existing issues in their respective repositories. Warning : Before exploring any SDK, please read the Before You Start section . Also, remember to check the API Reference Documentation first to verify if the problem is in SDK implementation or in the API itself. Cordova Dart , published on pub.dev Elixir Go NodeJS PHP Laravel Python , published on pypi React Native Ruby Java Rust R","title":"SDKs"},{"location":"api/explain-knowledge-panels/","text":"Explanation on Knowledge panels # The Open Food Facts API allows clients (such as the Open Food Facts website and mobile app) to request ready-to-display information about an object (such as a product or a facet like a category). Clients do not have to know in advance what kind of information is displayed (for example - the ingredients of a product, nutrition data, Nutri-Score or Eco-Score). They only have to know how to display essential data types such as texts, grades, images, and tables. Knowledge panels in action on the mobile app Main elements are panels, which in turn will contain elements. Elements are typically text_element , image_element , map_element . Some panels are grouping panels together, forming a hierarchy. The structure of the knowledge panels data returned by the API is described in the knowledge panels JSON schema . See the reference documentation for Getting Knowledge panels for a specific product by barcode .","title":"Explanation on Knowledge panels"},{"location":"api/explain-knowledge-panels/#explanation-on-knowledge-panels","text":"The Open Food Facts API allows clients (such as the Open Food Facts website and mobile app) to request ready-to-display information about an object (such as a product or a facet like a category). Clients do not have to know in advance what kind of information is displayed (for example - the ingredients of a product, nutrition data, Nutri-Score or Eco-Score). They only have to know how to display essential data types such as texts, grades, images, and tables. Knowledge panels in action on the mobile app Main elements are panels, which in turn will contain elements. Elements are typically text_element , image_element , map_element . Some panels are grouping panels together, forming a hierarchy. The structure of the knowledge panels data returned by the API is described in the knowledge panels JSON schema . See the reference documentation for Getting Knowledge panels for a specific product by barcode .","title":"Explanation on Knowledge panels"},{"location":"api/how-to-download-images/","text":"How to download product images # All images can be found on https://images.openfoodfacts.org/images/products/ . Images of a product are stored in a single directory. The path of this directory can be inferred easily from the product barcode. If the product barcode length is lower or equal to 8 (ex: \\\"22222222\\\"), the directory path is simply the barcode: all images can be found on https://images.openfoodfacts.org/images/products/{barcode} . Otherwise, the following regex is used to split the barcode into subfolders: r\"^(...)(...)(...)(.*)$\" . For example, the barcode 3435660768163 is split as follows: 343/566/076/8163, and all images of the products can be found on https://images.openfoodfacts.org/images/products/343/566/076/8163 . To get the image file names, we have to use the database dump or the API. All images information are stored in the images field. For product 3168930010883 , we have: { \"4\" : { \"uploader\" : \"openfoodfacts-contributors\" , \"uploaded_t\" : 1548685211 , \"sizes\" : { \"400\" : { \"h\" : 400 , \"w\" : 300 }, \"100\" : { \"w\" : 75 , \"h\" : 100 }, \"full\" : { \"h\" : 3174 , \"w\" : 2380 } } }, \"3\" : { \"uploader\" : \"openfoodfacts-contributors\" , \"uploaded_t\" : 1537002125 , \"sizes\" : { \"full\" : { \"h\" : 3302 , \"w\" : 2476 }, \"100\" : { \"h\" : 100 , \"w\" : 75 }, \"400\" : { \"w\" : 300 , \"h\" : 400 } } }, \"ingredients_fr\" : { \"rev\" : \"7\" , \"orientation\" : \"0\" , \"ocr\" : 1 , \"imgid\" : \"2\" , \"y2\" : null , \"white_magic\" : \"0\" , \"angle\" : null , \"x1\" : null , \"x2\" : null , \"geometry\" : \"0x0-0-0\" , \"normalize\" : \"0\" , \"y1\" : null , \"sizes\" : { \"100\" : { \"h\" : 100 , \"w\" : 75 }, \"400\" : { \"w\" : 300 , \"h\" : 400 }, \"200\" : { \"w\" : 150 , \"h\" : 200 }, \"full\" : { \"h\" : 1200 , \"w\" : 900 } } }, \"nutrition_fr\" : { \"sizes\" : { \"200\" : { \"h\" : 200 , \"w\" : 150 }, \"full\" : { \"w\" : 2476 , \"h\" : 3302 }, \"100\" : { \"w\" : 75 , \"h\" : 100 }, \"400\" : { \"w\" : 300 , \"h\" : 400 } }, \"y1\" : \"-1\" , \"normalize\" : null , \"x2\" : \"-1\" , \"geometry\" : \"0x0--8--8\" , \"x1\" : \"-1\" , \"angle\" : 0 , \"imgid\" : \"3\" , \"white_magic\" : null , \"y2\" : \"-1\" , \"ocr\" : 1 , \"orientation\" : \"0\" , \"rev\" : \"11\" }, \"1\" : { \"sizes\" : { \"full\" : { \"w\" : 850 , \"h\" : 1200 }, \"100\" : { \"h\" : 100 , \"w\" : 71 }, \"400\" : { \"h\" : 400 , \"w\" : 283 } }, \"uploader\" : \"kiliweb\" , \"uploaded_t\" : \"1527184614\" }, \"2\" : { \"sizes\" : { \"100\" : { \"h\" : 100 , \"w\" : 75 }, \"400\" : { \"h\" : 400 , \"w\" : 300 }, \"full\" : { \"h\" : 1200 , \"w\" : 900 } }, \"uploader\" : \"kiliweb\" , \"uploaded_t\" : \"1527184615\" }, \"front_fr\" : { \"x1\" : null , \"angle\" : null , \"y2\" : null , \"white_magic\" : \"0\" , \"imgid\" : \"1\" , \"rev\" : \"4\" , \"sizes\" : { \"200\" : { \"w\" : 142 , \"h\" : 200 }, \"full\" : { \"w\" : 850 , \"h\" : 1200 }, \"400\" : { \"h\" : 400 , \"w\" : 283 }, \"100\" : { \"w\" : 71 , \"h\" : 100 } }, \"y1\" : null , \"normalize\" : \"0\" , \"geometry\" : \"0x0-0-0\" , \"x2\" : null } } The keys of the map are the keys of the images. These keys can be: digits: the image is the raw image sent by the contributor. selected images: front_{lang} , nutrition_{lang} and ingredients_{lang} , selected as front, nutrition and ingredients images respectively for lang . Here, lang is a 2-letter ISO 639-1 language code (fr, en, es,...). Each image is available in different resolutions: \\\"100\\\", \\\"200\\\", \\\"400\\\" or \\\"full\\\", each corresponding to image height (\\\"full\\\" means not resized). The available resolutions can be found in the sizes subfield. Selected images have additional fields: rev (as revision) indicates the revision number of the image to use (each time a new image is selected, cropped or rotated, a new image with an incremented rev is generated). imgid , the image ID of the raw image used to generate the selected image. angle , x1 , x2 , y1 , y2 : rotation angle and cropping coordinates. For selected images, the file name is the image key followed by the revision number and the resolution: front_fr.1.400.jpg . For raw images, the file name is either the image ID ( 1.jpg ) or the image ID followed by the resolution ( 1.100.jpg ). To get the full URL, simply concatenate the product directory path and the image name. If you want to download a significant number of images, let us know before on our Slack and don\\'t be too eager to keep our servers safe!","title":"How to download product images"},{"location":"api/how-to-download-images/#how-to-download-product-images","text":"All images can be found on https://images.openfoodfacts.org/images/products/ . Images of a product are stored in a single directory. The path of this directory can be inferred easily from the product barcode. If the product barcode length is lower or equal to 8 (ex: \\\"22222222\\\"), the directory path is simply the barcode: all images can be found on https://images.openfoodfacts.org/images/products/{barcode} . Otherwise, the following regex is used to split the barcode into subfolders: r\"^(...)(...)(...)(.*)$\" . For example, the barcode 3435660768163 is split as follows: 343/566/076/8163, and all images of the products can be found on https://images.openfoodfacts.org/images/products/343/566/076/8163 . To get the image file names, we have to use the database dump or the API. All images information are stored in the images field. For product 3168930010883 , we have: { \"4\" : { \"uploader\" : \"openfoodfacts-contributors\" , \"uploaded_t\" : 1548685211 , \"sizes\" : { \"400\" : { \"h\" : 400 , \"w\" : 300 }, \"100\" : { \"w\" : 75 , \"h\" : 100 }, \"full\" : { \"h\" : 3174 , \"w\" : 2380 } } }, \"3\" : { \"uploader\" : \"openfoodfacts-contributors\" , \"uploaded_t\" : 1537002125 , \"sizes\" : { \"full\" : { \"h\" : 3302 , \"w\" : 2476 }, \"100\" : { \"h\" : 100 , \"w\" : 75 }, \"400\" : { \"w\" : 300 , \"h\" : 400 } } }, \"ingredients_fr\" : { \"rev\" : \"7\" , \"orientation\" : \"0\" , \"ocr\" : 1 , \"imgid\" : \"2\" , \"y2\" : null , \"white_magic\" : \"0\" , \"angle\" : null , \"x1\" : null , \"x2\" : null , \"geometry\" : \"0x0-0-0\" , \"normalize\" : \"0\" , \"y1\" : null , \"sizes\" : { \"100\" : { \"h\" : 100 , \"w\" : 75 }, \"400\" : { \"w\" : 300 , \"h\" : 400 }, \"200\" : { \"w\" : 150 , \"h\" : 200 }, \"full\" : { \"h\" : 1200 , \"w\" : 900 } } }, \"nutrition_fr\" : { \"sizes\" : { \"200\" : { \"h\" : 200 , \"w\" : 150 }, \"full\" : { \"w\" : 2476 , \"h\" : 3302 }, \"100\" : { \"w\" : 75 , \"h\" : 100 }, \"400\" : { \"w\" : 300 , \"h\" : 400 } }, \"y1\" : \"-1\" , \"normalize\" : null , \"x2\" : \"-1\" , \"geometry\" : \"0x0--8--8\" , \"x1\" : \"-1\" , \"angle\" : 0 , \"imgid\" : \"3\" , \"white_magic\" : null , \"y2\" : \"-1\" , \"ocr\" : 1 , \"orientation\" : \"0\" , \"rev\" : \"11\" }, \"1\" : { \"sizes\" : { \"full\" : { \"w\" : 850 , \"h\" : 1200 }, \"100\" : { \"h\" : 100 , \"w\" : 71 }, \"400\" : { \"h\" : 400 , \"w\" : 283 } }, \"uploader\" : \"kiliweb\" , \"uploaded_t\" : \"1527184614\" }, \"2\" : { \"sizes\" : { \"100\" : { \"h\" : 100 , \"w\" : 75 }, \"400\" : { \"h\" : 400 , \"w\" : 300 }, \"full\" : { \"h\" : 1200 , \"w\" : 900 } }, \"uploader\" : \"kiliweb\" , \"uploaded_t\" : \"1527184615\" }, \"front_fr\" : { \"x1\" : null , \"angle\" : null , \"y2\" : null , \"white_magic\" : \"0\" , \"imgid\" : \"1\" , \"rev\" : \"4\" , \"sizes\" : { \"200\" : { \"w\" : 142 , \"h\" : 200 }, \"full\" : { \"w\" : 850 , \"h\" : 1200 }, \"400\" : { \"h\" : 400 , \"w\" : 283 }, \"100\" : { \"w\" : 71 , \"h\" : 100 } }, \"y1\" : null , \"normalize\" : \"0\" , \"geometry\" : \"0x0-0-0\" , \"x2\" : null } } The keys of the map are the keys of the images. These keys can be: digits: the image is the raw image sent by the contributor. selected images: front_{lang} , nutrition_{lang} and ingredients_{lang} , selected as front, nutrition and ingredients images respectively for lang . Here, lang is a 2-letter ISO 639-1 language code (fr, en, es,...). Each image is available in different resolutions: \\\"100\\\", \\\"200\\\", \\\"400\\\" or \\\"full\\\", each corresponding to image height (\\\"full\\\" means not resized). The available resolutions can be found in the sizes subfield. Selected images have additional fields: rev (as revision) indicates the revision number of the image to use (each time a new image is selected, cropped or rotated, a new image with an incremented rev is generated). imgid , the image ID of the raw image used to generate the selected image. angle , x1 , x2 , y1 , y2 : rotation angle and cropping coordinates. For selected images, the file name is the image key followed by the revision number and the resolution: front_fr.1.400.jpg . For raw images, the file name is either the image ID ( 1.jpg ) or the image ID followed by the resolution ( 1.100.jpg ). To get the full URL, simply concatenate the product directory path and the image name. If you want to download a significant number of images, let us know before on our Slack and don\\'t be too eager to keep our servers safe!","title":"How to download product images"},{"location":"api/intro-robotoff/","text":"Introduction to the Robotoff Project # The Robotoff project is intended to complete missing product information by prompting users to confirm predictions inferred by Artificial Intelligence algorithms. These algorithms are calculated based on \"insights\", which are facts about a product that have been extracted or deduced from the product pictures, ingredients, categories, labels, etc. Robotoff documentation # The documentation for Robotoff is located here: General Documentation API Documentation (OpenApi)","title":"Introduction to the Robotoff Project"},{"location":"api/intro-robotoff/#introduction-to-the-robotoff-project","text":"The Robotoff project is intended to complete missing product information by prompting users to confirm predictions inferred by Artificial Intelligence algorithms. These algorithms are calculated based on \"insights\", which are facts about a product that have been extracted or deduced from the product pictures, ingredients, categories, labels, etc.","title":"Introduction to the Robotoff Project"},{"location":"api/intro-robotoff/#robotoff-documentation","text":"The documentation for Robotoff is located here: General Documentation API Documentation (OpenApi)","title":"Robotoff documentation"},{"location":"api/ref-cheatsheet/","text":"Reference: API CheatSheet # This reference cheatsheet gives you a quick reminder to send requests to the OFF API. If you are new to API usage you might look at the tutorial . Also, refer to the API reference documentation for complete information. Add/Edit an Existing Product # Indicate the absence of nutrition facts # no_nutrition_data=on (indicates if the nutrition facts are not indicated on the food label) Add nutrition facts values, units and base # nutrition_data_per=100g OR nutrition_data_per=serving serving_size=38g nutriment_energy=450 nutriment_energy_unit=kJ Adding values to a field that is already filled # You just have to prefix add_ before the name of the field add_categories add_labels add_brands Search for Products # Reference documentation for search API Get data for a list of products # You can use comma to seperate multiple values of a query paremeter. This allows you to make bulk requests. The product result can also be limited to specified data using fields . https://world.openfoodfacts.org/api/v2/search?code=3263859883713,8437011606013,6111069000451&fields=code,product_name","title":"Reference: API CheatSheet"},{"location":"api/ref-cheatsheet/#reference-api-cheatsheet","text":"This reference cheatsheet gives you a quick reminder to send requests to the OFF API. If you are new to API usage you might look at the tutorial . Also, refer to the API reference documentation for complete information.","title":"Reference: API CheatSheet"},{"location":"api/ref-cheatsheet/#addedit-an-existing-product","text":"","title":"Add/Edit an Existing Product"},{"location":"api/ref-cheatsheet/#indicate-the-absence-of-nutrition-facts","text":"no_nutrition_data=on (indicates if the nutrition facts are not indicated on the food label)","title":"Indicate the absence of nutrition facts"},{"location":"api/ref-cheatsheet/#add-nutrition-facts-values-units-and-base","text":"nutrition_data_per=100g OR nutrition_data_per=serving serving_size=38g nutriment_energy=450 nutriment_energy_unit=kJ","title":"Add nutrition facts values, units and base"},{"location":"api/ref-cheatsheet/#adding-values-to-a-field-that-is-already-filled","text":"You just have to prefix add_ before the name of the field add_categories add_labels add_brands","title":"Adding values to a field that is already filled"},{"location":"api/ref-cheatsheet/#search-for-products","text":"Reference documentation for search API","title":"Search for Products"},{"location":"api/ref-cheatsheet/#get-data-for-a-list-of-products","text":"You can use comma to seperate multiple values of a query paremeter. This allows you to make bulk requests. The product result can also be limited to specified data using fields . https://world.openfoodfacts.org/api/v2/search?code=3263859883713,8437011606013,6111069000451&fields=code,product_name","title":"Get data for a list of products"},{"location":"api/ref-v2/","text":"Reference OpenAPI documentation for v2 # See api.yml for edition. Do not write anything here, it is meant to be overwritten by html generated from api.yml","title":"Reference OpenAPI documentation for v2"},{"location":"api/ref-v2/#reference-openapi-documentation-for-v2","text":"See api.yml for edition. Do not write anything here, it is meant to be overwritten by html generated from api.yml","title":"Reference OpenAPI documentation for v2"},{"location":"api/ref-v3/","text":"Reference OpenAPI documentation for v3 # See api-v3.yml for edition. Do not write anything here, it is meant to be overwritten by html generated from api-v3.yml","title":"Reference OpenAPI documentation for v3"},{"location":"api/ref-v3/#reference-openapi-documentation-for-v3","text":"See api-v3.yml for edition. Do not write anything here, it is meant to be overwritten by html generated from api-v3.yml","title":"Reference OpenAPI documentation for v3"},{"location":"api/tutorial-dev-journey/","text":"Tutorial - developer journey # Meet Dave. Dave is an active Open Food Facts contributor and a developer who wants to build HealthyFoodChoices , an Android app aimed at conscious consumers that buy healthy products. HealthyFoodChoices will query Open Food Facts API and provide information on healthy foods available in the place users are living in. Users can narrow down the results by applying different filters and save their search criteria so that the app shows them the products that match their preferences next time they use it. To identify the potential users' needs, Dave has met with some conscious consumers. Anna is a 25-year old New Yorker who doesn't drink soda , but her nephew does . She wants to compare the nutrition facts of two cola brands , and its variants ( diet , zero , and so on) to decide which one to buy. Stefano is a 36-year old Italian who follows a plant-based diet and wants to avoid the intake of palm oil . He's looking for a breakfast cereal brand that does not use palm oil nor additives and has a great nutriscore (A) . Dev Journey 1: Comparing sodas for Anna Dev Journey 2: Finding healthy breakfast cereals for Stefano Dev Journey 3: Adding missing products Dev Journey 4: Get the Nutri-Score Dev Journey 5 : Get the Eco-Score Dev Journey 6: Get ingredient related analysis on new or existing products (Nova, allergens, additives\u2026)","title":"Tutorial - developer journey"},{"location":"api/tutorial-dev-journey/#tutorial-developer-journey","text":"Meet Dave. Dave is an active Open Food Facts contributor and a developer who wants to build HealthyFoodChoices , an Android app aimed at conscious consumers that buy healthy products. HealthyFoodChoices will query Open Food Facts API and provide information on healthy foods available in the place users are living in. Users can narrow down the results by applying different filters and save their search criteria so that the app shows them the products that match their preferences next time they use it. To identify the potential users' needs, Dave has met with some conscious consumers. Anna is a 25-year old New Yorker who doesn't drink soda , but her nephew does . She wants to compare the nutrition facts of two cola brands , and its variants ( diet , zero , and so on) to decide which one to buy. Stefano is a 36-year old Italian who follows a plant-based diet and wants to avoid the intake of palm oil . He's looking for a breakfast cereal brand that does not use palm oil nor additives and has a great nutriscore (A) . Dev Journey 1: Comparing sodas for Anna Dev Journey 2: Finding healthy breakfast cereals for Stefano Dev Journey 3: Adding missing products Dev Journey 4: Get the Nutri-Score Dev Journey 5 : Get the Eco-Score Dev Journey 6: Get ingredient related analysis on new or existing products (Nova, allergens, additives\u2026)","title":"Tutorial - developer journey"},{"location":"api/tutorial-off-api/","text":"Tutorial on using the Open Food Facts API # Welcome to this tutorial on basic usage of Open Food Facts API. Fist, be sure to see our Introduction to the API . Scan A Product To Get Nutri-score # This basic tutorial shows you can get the Nutri-score of a product, for instance, to display it in a mobile app after scanning the product barcode. Let's use Nutella Ferrero as the product example for this tutorial. To get a product nutriscore, send a request to the Get A Product By Barcode endpoint. Authentication # Usually, no authentication is required to query Get A Product Nutri-score. However, there is a basic auth to avoid content indexation in the staging environment(which is used throughout this tutorial). For more details, visit the Open Food Facts API Environment . Describing the Get Request # Make a GET request to the Get A Product By Barcode endpoint. https://world.openfoodfacts.net/api/v2/product/{barcode} The {barcode} is the barcode number of the product you are trying to get. The barcode for Nutella Ferrero is 3017624010701 . Then the request path to get product data for Nutella Ferrero will look like this: https://world.openfoodfacts.net/api/v2/product/3017624010701 The response returns every data about Nutella Ferrero on the database. To get the nutriscore, we need to limit the response by specifying the nutriscore field, which is the nutrition_grades and product_name . Query Parameters # To limit the response of the Get A Product By Barcode response, use query parameters to specify the product fields to be returned. In this example, you need one query parameter called field with the value product_name,nutrition_grades . The request path will now look like this: https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data Nutri-Score Response # The response returned contains an object of the code , product , status_verbose , and status . The product object contains the fields specified in the query: the product_name and the nutrition_grades . The status also states if the product was found or not. { \"code\" : \"3017624010701\" , \"product\" : { \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } Nutri-Score Computation # If you would like to be able to show how the score is computed, add some extra fields like nutriscore_data and nutriments . The request path to get the Nutri-Score computation for Nutella-Ferroro will be : https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data,nutriments,nutrition_grades The product object in the response now contains the extra fields to show how the nutriscore was computed. { \"code\" : \"3017624010701\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 57.5 , \"carbohydrates_100g\" : 57.5 , \"carbohydrates_unit\" : \"g\" , \"carbohydrates_value\" : 57.5 , \"energy\" : 2255 , \"energy-kcal\" : 539 , \"energy-kcal_100g\" : 539 , \"energy-kcal_unit\" : \"kcal\" , ... , ... , \"sugars\" : 56.3 , \"sugars_100g\" : 56.3 , \"sugars_unit\" : \"g\" , \"sugars_value\" : 56.3 }, \"nutriscore_data\" : { \"energy\" : 2255 , \"energy_points\" : 6 , \"energy_value\" : 2255 , ... , ... , \"sugars_points\" : 10 , \"sugars_value\" : 56.3 }, \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Get A Product By Barcode . Completing products to get the Nutri-Score # Products without a Nutri-Score # When these fields are missing in a nutriscore computation response, it signifies that the product does not have a Nutri-Score computation due to some missing nutrition data. Let's look at the 100% Real Orange Juice . If the product nutrition data is missing some fields, you can volunteer and contribute to it by getting the missing tags and writing to the OFF API to add them. To know the missing tags, check the misc-tags field from the product response. https://world.openfoodfacts.net/api/v2/product/0180411000803/100-real-orange-juice?fields=misc_tags The response shows the missing fields and category needed to compute the Nutri-Score. { \"code\" : \"0180411000803\" , \"product\" : { \"misc_tags\" : [ \"en:nutriscore-not-computed\" , \"en:nutriscore-missing-category\" , \"en:nutrition-not-enough-data-to-compute-nutrition-score\" , \"en:nutriscore-missing-nutrition-data\" , \"en:nutriscore-missing-nutrition-data-sodium\" , \"en:ecoscore-extended-data-not-computed\" , \"en:ecoscore-not-computed\" , \"en:main-countries-new-product\" ] }, \"status\" : 1 , \"status_verbose\" : \"product found\" } The sample response above for 100% Real Orange Juice misc_tags shows that the Nutri-Score is missing category ( en:nutriscore-missing-category ) and sodium(salt) ( en:nutriscore-missing-nutrition-data-sodium ). Now you can write to the OFF API to provide these nutriment data (if you have them) so that the Nutri-Score can be computed. Write data to make Nutri-Score computation possible # The WRITE operations in the OFF API require authentication. Therefore you need a valid user_id and password to write the missing nutriment data to 100% Real Orange Juice. Sign up on the Open Food Facts App to get your user_id and password if you don't have one. To write data to a product, make a POST request to the Add or Edit A Product endpoint. https://world.openfoodfacts.net/cgi/product_jqm2.pl Add your valid user_id and password as body parameters to your request for authentication. The code (barcode of the product to be added/edited), user_id , and password are required when adding or editing a product. Then, include other product data to be added in the request body. To write sodium and category to 100% Real Orange Juice so that the Nutri-Score can be computed, the request body should contain these fields : Key Value Description user_id *** A valid user_id password *** A valid password code 0180411000803 The barcode of the product to be added/edited nutriment_sodium 0.015 Amount of sodium nutriment_sodium_unit g Unit of sodium relative to the amount categories Orange Juice Category of the Product Using curl: curl -XPOST -u off:off -x POST https://world.openfoodfacts.net/cgi/product_jqm2.pl \\ -F user_id = your_user_id -F password = your_password \\ -F code = 0180411000803 -F nutriment_sodium = 0 .015 -F nutriment_sodium_unit = g -F categories = \"Orange Juice\" If the request is succesful, it returns a response that indicated that the fields have been saved. { \"status_verbose\" : \"fields saved\" , \"status\" : 1 } Read newly computed Nutri-Score # Now, let's check if the Nutri-Score for 100% Real Orange Juice has been computed now that we have provided the missing data. Make a GET request to https://world.openfoodfacts.net/api/v2/product/0180411000803?fields=product_name,nutriscore_data,nutriments,nutrition_grades for Nutri-Score of 100% Real Orange Juice. The response now contains the Nutri-Score computation: { \"code\" : \"0180411000803\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 11.864406779661 , . . . \"sugars_unit\" : \"g\" , \"sugars_value\" : 11.864406779661 }, \"nutriscore_data\" : { \"energy\" : 195 , \"energy_points\" : 7 , \"energy_value\" : 195 , . . . \"sugars_value\" : 11.86 }, \"nutrition_grades\" : \"c\" , \"product_name\" : \"100% Real Orange Juice\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Add or Edit A Product You can also check the reference cheatsheet to know how to add/edit other types of product data. Search for a Product by Nutri-score # Using the Open Food Facts API, you can filter products based on different criteria. To search for products in the Orange Juice category with a nutrition_grade of c , query the Search for Products endpoint . Describing the Search Request # Make a GET request to the Search for Products endpoint. https://world.openfoodfacts.org/api/v2/search Add the search criteria used to filter the products as query parameters. For Orange Juice with a nutrition_grade of c , add query parameters categories_tags_en to filter Orange Juice while nutrition_grades_tags to filter c . The response will return all the products in the database with the category Orange Juice and nutrition_grade c . https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&nutrition_grades_tags=c To limit the response, add fields to the query parameters to specify the fields to be returned in each product object response. For this tutorial, limit the response to code , product_name , nutrition_grades , and categories_tags_en . https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&nutrition_grades_tags=c&fields=code,nutrition_grades,categories_tags_en The response returns all products that belong to the Orange Juice category, with the nutrition_grade \"c\" and limits each product object response to only the specified fields. It also returns the count(total number) of products that match the search criteria. { \"count\" : 1629 , \"page\" : 1 , \"page_count\" : 24 , \"page_size\" : 24 , \"products\" : [ { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Concentrated fruit juices\" , \"Orange juices\" , \"Concentrated orange juices\" ], \"code\" : \"3123340008288\" , \"nutrition_grades\" : \"c\" }, . . . { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Non-Alcoholic beverages\" , \"Orange juices\" , \"Squeezed juices\" , \"Squeezed orange juices\" ], \"code\" : \"3608580844136\" , \"nutrition_grades\" : \"c\" } ], \"skip\" : 0 } Sorting Search Response # You can proceed to also sort the search response by different fields, for example, sort by the product that was modified last or even by the product_name. Now, let's sort the products with Orange Juice and a nutrition_grade of \"c\" by when they were last modified. To sort the search response, add the sort_by with value last_modified_t as a query parameter to the request. https://world.openfoodfacts.net/api/v2/search?nutrition_grades_tags=c&fields=code,nutrition_grades,categories_tags_en&categories_tags_en=Orange Juice&sort_by=last_modified_t The date that each product was last modified is now used to order the product response. { \"count\" : 1629 , \"page\" : 1 , \"page_count\" : 24 , \"page_size\" : 24 , \"products\" : [ { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Orange juices\" ], \"code\" : \"3800014268048\" , \"nutrition_grades\" : \"c\" }, ' ' ' { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Orange juices\" , \"Squeezed juices\" , \"Squeezed orange juices\" ], \"code\" : \"4056489641018\" , \"nutrition_grades\" : \"c\" } ], \"skip\" : 0 } To see other examples of sorting a search response, see the reference documentation for Search for Products .","title":"Tutorial on using the Open Food Facts API"},{"location":"api/tutorial-off-api/#tutorial-on-using-the-open-food-facts-api","text":"Welcome to this tutorial on basic usage of Open Food Facts API. Fist, be sure to see our Introduction to the API .","title":"Tutorial on using the Open Food Facts API"},{"location":"api/tutorial-off-api/#scan-a-product-to-get-nutri-score","text":"This basic tutorial shows you can get the Nutri-score of a product, for instance, to display it in a mobile app after scanning the product barcode. Let's use Nutella Ferrero as the product example for this tutorial. To get a product nutriscore, send a request to the Get A Product By Barcode endpoint.","title":"Scan A Product To Get Nutri-score"},{"location":"api/tutorial-off-api/#authentication","text":"Usually, no authentication is required to query Get A Product Nutri-score. However, there is a basic auth to avoid content indexation in the staging environment(which is used throughout this tutorial). For more details, visit the Open Food Facts API Environment .","title":"Authentication"},{"location":"api/tutorial-off-api/#describing-the-get-request","text":"Make a GET request to the Get A Product By Barcode endpoint. https://world.openfoodfacts.net/api/v2/product/{barcode} The {barcode} is the barcode number of the product you are trying to get. The barcode for Nutella Ferrero is 3017624010701 . Then the request path to get product data for Nutella Ferrero will look like this: https://world.openfoodfacts.net/api/v2/product/3017624010701 The response returns every data about Nutella Ferrero on the database. To get the nutriscore, we need to limit the response by specifying the nutriscore field, which is the nutrition_grades and product_name .","title":"Describing the Get Request"},{"location":"api/tutorial-off-api/#query-parameters","text":"To limit the response of the Get A Product By Barcode response, use query parameters to specify the product fields to be returned. In this example, you need one query parameter called field with the value product_name,nutrition_grades . The request path will now look like this: https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data","title":"Query Parameters"},{"location":"api/tutorial-off-api/#nutri-score-response","text":"The response returned contains an object of the code , product , status_verbose , and status . The product object contains the fields specified in the query: the product_name and the nutrition_grades . The status also states if the product was found or not. { \"code\" : \"3017624010701\" , \"product\" : { \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" }","title":"Nutri-Score Response"},{"location":"api/tutorial-off-api/#nutri-score-computation","text":"If you would like to be able to show how the score is computed, add some extra fields like nutriscore_data and nutriments . The request path to get the Nutri-Score computation for Nutella-Ferroro will be : https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data,nutriments,nutrition_grades The product object in the response now contains the extra fields to show how the nutriscore was computed. { \"code\" : \"3017624010701\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 57.5 , \"carbohydrates_100g\" : 57.5 , \"carbohydrates_unit\" : \"g\" , \"carbohydrates_value\" : 57.5 , \"energy\" : 2255 , \"energy-kcal\" : 539 , \"energy-kcal_100g\" : 539 , \"energy-kcal_unit\" : \"kcal\" , ... , ... , \"sugars\" : 56.3 , \"sugars_100g\" : 56.3 , \"sugars_unit\" : \"g\" , \"sugars_value\" : 56.3 }, \"nutriscore_data\" : { \"energy\" : 2255 , \"energy_points\" : 6 , \"energy_value\" : 2255 , ... , ... , \"sugars_points\" : 10 , \"sugars_value\" : 56.3 }, \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Get A Product By Barcode .","title":"Nutri-Score Computation"},{"location":"api/tutorial-off-api/#completing-products-to-get-the-nutri-score","text":"","title":"Completing products to get the Nutri-Score"},{"location":"api/tutorial-off-api/#products-without-a-nutri-score","text":"When these fields are missing in a nutriscore computation response, it signifies that the product does not have a Nutri-Score computation due to some missing nutrition data. Let's look at the 100% Real Orange Juice . If the product nutrition data is missing some fields, you can volunteer and contribute to it by getting the missing tags and writing to the OFF API to add them. To know the missing tags, check the misc-tags field from the product response. https://world.openfoodfacts.net/api/v2/product/0180411000803/100-real-orange-juice?fields=misc_tags The response shows the missing fields and category needed to compute the Nutri-Score. { \"code\" : \"0180411000803\" , \"product\" : { \"misc_tags\" : [ \"en:nutriscore-not-computed\" , \"en:nutriscore-missing-category\" , \"en:nutrition-not-enough-data-to-compute-nutrition-score\" , \"en:nutriscore-missing-nutrition-data\" , \"en:nutriscore-missing-nutrition-data-sodium\" , \"en:ecoscore-extended-data-not-computed\" , \"en:ecoscore-not-computed\" , \"en:main-countries-new-product\" ] }, \"status\" : 1 , \"status_verbose\" : \"product found\" } The sample response above for 100% Real Orange Juice misc_tags shows that the Nutri-Score is missing category ( en:nutriscore-missing-category ) and sodium(salt) ( en:nutriscore-missing-nutrition-data-sodium ). Now you can write to the OFF API to provide these nutriment data (if you have them) so that the Nutri-Score can be computed.","title":"Products without a Nutri-Score"},{"location":"api/tutorial-off-api/#write-data-to-make-nutri-score-computation-possible","text":"The WRITE operations in the OFF API require authentication. Therefore you need a valid user_id and password to write the missing nutriment data to 100% Real Orange Juice. Sign up on the Open Food Facts App to get your user_id and password if you don't have one. To write data to a product, make a POST request to the Add or Edit A Product endpoint. https://world.openfoodfacts.net/cgi/product_jqm2.pl Add your valid user_id and password as body parameters to your request for authentication. The code (barcode of the product to be added/edited), user_id , and password are required when adding or editing a product. Then, include other product data to be added in the request body. To write sodium and category to 100% Real Orange Juice so that the Nutri-Score can be computed, the request body should contain these fields : Key Value Description user_id *** A valid user_id password *** A valid password code 0180411000803 The barcode of the product to be added/edited nutriment_sodium 0.015 Amount of sodium nutriment_sodium_unit g Unit of sodium relative to the amount categories Orange Juice Category of the Product Using curl: curl -XPOST -u off:off -x POST https://world.openfoodfacts.net/cgi/product_jqm2.pl \\ -F user_id = your_user_id -F password = your_password \\ -F code = 0180411000803 -F nutriment_sodium = 0 .015 -F nutriment_sodium_unit = g -F categories = \"Orange Juice\" If the request is succesful, it returns a response that indicated that the fields have been saved. { \"status_verbose\" : \"fields saved\" , \"status\" : 1 }","title":"Write data to make Nutri-Score computation possible"},{"location":"api/tutorial-off-api/#read-newly-computed-nutri-score","text":"Now, let's check if the Nutri-Score for 100% Real Orange Juice has been computed now that we have provided the missing data. Make a GET request to https://world.openfoodfacts.net/api/v2/product/0180411000803?fields=product_name,nutriscore_data,nutriments,nutrition_grades for Nutri-Score of 100% Real Orange Juice. The response now contains the Nutri-Score computation: { \"code\" : \"0180411000803\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 11.864406779661 , . . . \"sugars_unit\" : \"g\" , \"sugars_value\" : 11.864406779661 }, \"nutriscore_data\" : { \"energy\" : 195 , \"energy_points\" : 7 , \"energy_value\" : 195 , . . . \"sugars_value\" : 11.86 }, \"nutrition_grades\" : \"c\" , \"product_name\" : \"100% Real Orange Juice\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Add or Edit A Product You can also check the reference cheatsheet to know how to add/edit other types of product data.","title":"Read newly computed Nutri-Score"},{"location":"api/tutorial-off-api/#search-for-a-product-by-nutri-score","text":"Using the Open Food Facts API, you can filter products based on different criteria. To search for products in the Orange Juice category with a nutrition_grade of c , query the Search for Products endpoint .","title":"Search for a Product by Nutri-score"},{"location":"api/tutorial-off-api/#describing-the-search-request","text":"Make a GET request to the Search for Products endpoint. https://world.openfoodfacts.org/api/v2/search Add the search criteria used to filter the products as query parameters. For Orange Juice with a nutrition_grade of c , add query parameters categories_tags_en to filter Orange Juice while nutrition_grades_tags to filter c . The response will return all the products in the database with the category Orange Juice and nutrition_grade c . https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&nutrition_grades_tags=c To limit the response, add fields to the query parameters to specify the fields to be returned in each product object response. For this tutorial, limit the response to code , product_name , nutrition_grades , and categories_tags_en . https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&nutrition_grades_tags=c&fields=code,nutrition_grades,categories_tags_en The response returns all products that belong to the Orange Juice category, with the nutrition_grade \"c\" and limits each product object response to only the specified fields. It also returns the count(total number) of products that match the search criteria. { \"count\" : 1629 , \"page\" : 1 , \"page_count\" : 24 , \"page_size\" : 24 , \"products\" : [ { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Concentrated fruit juices\" , \"Orange juices\" , \"Concentrated orange juices\" ], \"code\" : \"3123340008288\" , \"nutrition_grades\" : \"c\" }, . . . { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Non-Alcoholic beverages\" , \"Orange juices\" , \"Squeezed juices\" , \"Squeezed orange juices\" ], \"code\" : \"3608580844136\" , \"nutrition_grades\" : \"c\" } ], \"skip\" : 0 }","title":"Describing the Search Request"},{"location":"api/tutorial-off-api/#sorting-search-response","text":"You can proceed to also sort the search response by different fields, for example, sort by the product that was modified last or even by the product_name. Now, let's sort the products with Orange Juice and a nutrition_grade of \"c\" by when they were last modified. To sort the search response, add the sort_by with value last_modified_t as a query parameter to the request. https://world.openfoodfacts.net/api/v2/search?nutrition_grades_tags=c&fields=code,nutrition_grades,categories_tags_en&categories_tags_en=Orange Juice&sort_by=last_modified_t The date that each product was last modified is now used to order the product response. { \"count\" : 1629 , \"page\" : 1 , \"page_count\" : 24 , \"page_size\" : 24 , \"products\" : [ { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Orange juices\" ], \"code\" : \"3800014268048\" , \"nutrition_grades\" : \"c\" }, ' ' ' { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Orange juices\" , \"Squeezed juices\" , \"Squeezed orange juices\" ], \"code\" : \"4056489641018\" , \"nutrition_grades\" : \"c\" } ], \"skip\" : 0 } To see other examples of sorting a search response, see the reference documentation for Search for Products .","title":"Sorting Search Response"},{"location":"api/tutorial-uploading-photo-to-a-product/","text":"Tutorial on uploading images to the Open Food Facts API # This basic tutorial shows you how to upload an image of a product to the Open Food Facts API. Be sure to also read the introduction to the API Points to consider before uploading photos # Image license # Product images must be under the Creative Commons Attribution ShareAlike licence . That means you should either upload: photos that are your own work photos taken by your users, with their consent for this license (should be part of your service terms) photos already under this license or a compatible license ( cc-by , cc-0 or public domain) Image Quality # Uploading quality photos of a product, its ingredients, and the nutrition table is essential because it enables the Open Food Facts OCR system to retrieve important data to analyze the product. The minimal allowed size for photos is 640 x 160 px. Upload Behavior # In case you upload more than one photo of the front, the ingredients, the nutrition facts, or the product packaging components, beware that only the latest \"selected\" photo of each category will be displayed on the product page on the website and on the mobile application. The older ones are saved and can be \"selected\" by an API call or via the editing interface (website and mobile application). You can also upload some photos that are neither of that 4 categories, but they will not be displayed by default. However, all photos will be saved. Label Languages # Multilingual products have several photos based on the languages present on the packaging. You can specify the language by adding a lang code suffix to the image field . Authentication # The WRITE operations in the OFF API require authentication. Therefore you need a valid user_id and password to write the photo to 100% Real Orange Juice. Sign up on the Open Food Facts App to get your user_id and password if you dont have one. For more details, visit the Open Food Facts Authentication . Parameters # Code # The barcode of the product. Imagefield # imagefield indicates the type of image you are trying to upload for a particular product. It can be either of these: front , ingredients , nutrition , packaging or other . You can also specify the language in that image by adding a suffix of the language code to the imagefield value. For example \u2014 front_en , packaging_fr . ImageUpload # imageupload must contain the binary content of the image. This field name is dependent on the imagefield . It must be imgupload_ suffixed by the value of the imagefield stated earlier. Here are some examples: imgupload_front (if imagefield=front) imgupload_ingredients_fr (if imagefield=ingredients_fr) imgupload_nutrition (if imagefield=nutrition) imgupload_packaging (if imagefield=packaging) Describing the Post Request # To upload photos to a product, make a POST request to the Add a Photo to an Existing Product endpoint. https://off:off@world.openfoodfacts.net/cgi/product_image_upload.pl Upload Photo of a Product # Add a user_id and password as body parameters to the request for authentication. The code (barcode of the product to be updated) is required to indicate the product for the uploaded photo. Then, include other product data to be added in the request body. To add a new image for ingredients in English to 100% Real Orange Juice , the request body should contain these fields : Key Value Description user_id *** A valid user_id password *** A valid password code 0180411000803 The barcode of the product to be added/edited imagefield ingredients_en The type of image to be uploaded imgupload_ingredients_en file The binary content of the image of the product ingredients in English If the image is in the images/real-orange-juice-ingredients.jpg , we can use curl (thanks to the special '@' attributes, which enables reading from a file): curl -XPOST -u off:off https://world.openfoodfacts.net/cgi/product_image_upload.pl \\ -F user_id = your_user_id -F password = your_password \\ -F code = 0180411000803 -F imagefield = ingredients_en -F imgupload_ingredients_en = @images/real-orange-juice-ingredients.jpg If the request is successful, it returns a response that indicates that the fields have been saved. You will also get the new image id in imgid . { \"files\" : [ { \"url\" : \"/product/0180411000803/100%-real-orange-juice\" , \"filename\" : \"\" , \"name\" : \"100% Real Orange Juice\" , \"thumbnailUrl\" : \"/images/products/018/041/100/0803.jpg\" , \"code\" : \"0180411000803\" } ], \"image\" : { \"thumb_url\" : \"123.100.jpg\" , \"imgid\" : 123 , \"crop_url\" : \"123.400.jpg\" }, \"imgid\" : 123 , \"status\" : \"status ok\" , \"imagefield\" : \"ingredients_en\" , \"code\" : \"0180411000803\" } If the request is unsuccessful, the response returns \"status\": \"status not ok\" and an explanation in debug field.","title":"Tutorial on uploading images to the Open Food Facts API"},{"location":"api/tutorial-uploading-photo-to-a-product/#tutorial-on-uploading-images-to-the-open-food-facts-api","text":"This basic tutorial shows you how to upload an image of a product to the Open Food Facts API. Be sure to also read the introduction to the API","title":"Tutorial on uploading images to the Open Food Facts API"},{"location":"api/tutorial-uploading-photo-to-a-product/#points-to-consider-before-uploading-photos","text":"","title":"Points to consider before uploading photos"},{"location":"api/tutorial-uploading-photo-to-a-product/#image-license","text":"Product images must be under the Creative Commons Attribution ShareAlike licence . That means you should either upload: photos that are your own work photos taken by your users, with their consent for this license (should be part of your service terms) photos already under this license or a compatible license ( cc-by , cc-0 or public domain)","title":"Image license"},{"location":"api/tutorial-uploading-photo-to-a-product/#image-quality","text":"Uploading quality photos of a product, its ingredients, and the nutrition table is essential because it enables the Open Food Facts OCR system to retrieve important data to analyze the product. The minimal allowed size for photos is 640 x 160 px.","title":"Image Quality"},{"location":"api/tutorial-uploading-photo-to-a-product/#upload-behavior","text":"In case you upload more than one photo of the front, the ingredients, the nutrition facts, or the product packaging components, beware that only the latest \"selected\" photo of each category will be displayed on the product page on the website and on the mobile application. The older ones are saved and can be \"selected\" by an API call or via the editing interface (website and mobile application). You can also upload some photos that are neither of that 4 categories, but they will not be displayed by default. However, all photos will be saved.","title":"Upload Behavior"},{"location":"api/tutorial-uploading-photo-to-a-product/#label-languages","text":"Multilingual products have several photos based on the languages present on the packaging. You can specify the language by adding a lang code suffix to the image field .","title":"Label Languages"},{"location":"api/tutorial-uploading-photo-to-a-product/#authentication","text":"The WRITE operations in the OFF API require authentication. Therefore you need a valid user_id and password to write the photo to 100% Real Orange Juice. Sign up on the Open Food Facts App to get your user_id and password if you dont have one. For more details, visit the Open Food Facts Authentication .","title":"Authentication"},{"location":"api/tutorial-uploading-photo-to-a-product/#parameters","text":"","title":"Parameters"},{"location":"api/tutorial-uploading-photo-to-a-product/#code","text":"The barcode of the product.","title":"Code"},{"location":"api/tutorial-uploading-photo-to-a-product/#imagefield","text":"imagefield indicates the type of image you are trying to upload for a particular product. It can be either of these: front , ingredients , nutrition , packaging or other . You can also specify the language in that image by adding a suffix of the language code to the imagefield value. For example \u2014 front_en , packaging_fr .","title":"Imagefield"},{"location":"api/tutorial-uploading-photo-to-a-product/#imageupload","text":"imageupload must contain the binary content of the image. This field name is dependent on the imagefield . It must be imgupload_ suffixed by the value of the imagefield stated earlier. Here are some examples: imgupload_front (if imagefield=front) imgupload_ingredients_fr (if imagefield=ingredients_fr) imgupload_nutrition (if imagefield=nutrition) imgupload_packaging (if imagefield=packaging)","title":"ImageUpload"},{"location":"api/tutorial-uploading-photo-to-a-product/#describing-the-post-request","text":"To upload photos to a product, make a POST request to the Add a Photo to an Existing Product endpoint. https://off:off@world.openfoodfacts.net/cgi/product_image_upload.pl","title":"Describing the Post Request"},{"location":"api/tutorial-uploading-photo-to-a-product/#upload-photo-of-a-product","text":"Add a user_id and password as body parameters to the request for authentication. The code (barcode of the product to be updated) is required to indicate the product for the uploaded photo. Then, include other product data to be added in the request body. To add a new image for ingredients in English to 100% Real Orange Juice , the request body should contain these fields : Key Value Description user_id *** A valid user_id password *** A valid password code 0180411000803 The barcode of the product to be added/edited imagefield ingredients_en The type of image to be uploaded imgupload_ingredients_en file The binary content of the image of the product ingredients in English If the image is in the images/real-orange-juice-ingredients.jpg , we can use curl (thanks to the special '@' attributes, which enables reading from a file): curl -XPOST -u off:off https://world.openfoodfacts.net/cgi/product_image_upload.pl \\ -F user_id = your_user_id -F password = your_password \\ -F code = 0180411000803 -F imagefield = ingredients_en -F imgupload_ingredients_en = @images/real-orange-juice-ingredients.jpg If the request is successful, it returns a response that indicates that the fields have been saved. You will also get the new image id in imgid . { \"files\" : [ { \"url\" : \"/product/0180411000803/100%-real-orange-juice\" , \"filename\" : \"\" , \"name\" : \"100% Real Orange Juice\" , \"thumbnailUrl\" : \"/images/products/018/041/100/0803.jpg\" , \"code\" : \"0180411000803\" } ], \"image\" : { \"thumb_url\" : \"123.100.jpg\" , \"imgid\" : 123 , \"crop_url\" : \"123.400.jpg\" }, \"imgid\" : 123 , \"status\" : \"status ok\" , \"imagefield\" : \"ingredients_en\" , \"code\" : \"0180411000803\" } If the request is unsuccessful, the response returns \"status\": \"status not ok\" and an explanation in debug field.","title":"Upload Photo of a Product"},{"location":"dev/","text":"Introduction to Product Opener developer documentation # This documentation is for developers who wants to understand technical aspects of Product Opener. To use the API, see API Documentation The repository for the project is at https://github.com/openfoodfacts/openfoodfacts-server/ Some documentation to get you started: Quick start guide (Docker) Developer guide (Docker) Developer guide (Gitpod) Note: documentation follows the Di\u00e1taxis Framework","title":"Introduction to Product Opener developer documentation"},{"location":"dev/#introduction-to-product-opener-developer-documentation","text":"This documentation is for developers who wants to understand technical aspects of Product Opener. To use the API, see API Documentation The repository for the project is at https://github.com/openfoodfacts/openfoodfacts-server/ Some documentation to get you started: Quick start guide (Docker) Developer guide (Docker) Developer guide (Gitpod) Note: documentation follows the Di\u00e1taxis Framework","title":"Introduction to Product Opener developer documentation"},{"location":"dev/explain-packaging-data/","text":"Explanation on packaging data # This document explains how packaging data is currently added, updated and structured in the Open Food Facts database, and how it could be improved. Introduction # Types of packaging data # Food products typically have 1 or more packaging components (e.g. milk may have a bottle and a cap). For each product, we aim to have a comprehensive list of all its packaging components, with detailed information about each packaging component. Data about packaging components # For each packaging component, we want data for different attributes, like its shape (e.g. a bottle) and its size (e.g. plastic). There are many different attributes that can be interesting for specific uses. For instance, researchers in epidemiology are interested in knowing which packaging component is in contact with the food itself, and which one can be put in the microwave oven, so that they can study the long term effects of some plastics on health. Sources of packaging data # We can get packaging data from different sources: Users # Users of the Open Food Facts website and app, and users of 3rd party apps, can enter packaging data. Manufacturers # Some manufacturers send product data through GS1, which currently has limited support for packaging information (but this is likely to be improved in the years to come). Some manufacturers send us more detailed packaging data (e.g. recycling instructions) through the Producers Platform. Some manufacturers send us data used to compute the Eco-Score using the Eco-Score spreadsheet template, which has fields like \"Packaging 1\", \"Material 1\", \"Packaging 2\", \"Material 2\" etc. Product photos and machine learning # We can extract logos related to packaging, or parse the text recognized from product photos to recognize packaging information or recycling instructions. How packaging data is currently added, updated and structured in Open Food Facts # In Open Food Facts, we currently have a number of input fields related to packaging. The data in those fields is parsed and analyzed to create a structured list of packaging components with specific attributes. Current input fields # Packaging tag field (READ and WRITE) # At the start of Open Food Facts in 2012, we had a \"packaging\" tag field where users could enter comma separated free text entries about the packaging (e.g. \"Plastic\", \"Bag\" or \"Plastic bag\") in different languages. In 2020, we made this field a taxonomized field. As a result, we now store the language used to fill this field, so that we can match its value to the multilingual packaging taxonomy. So \"plastique\" in French will be mapped to the canonical \"en:plastic\" entry. Packaging information / recycling instructions text field (READ and WRITE) # In 2020, we also added a language specific field (\"packaging_text_[language code]\" e.g. \"packaging_text_en\" for English) to store free text data about the packaging. It can contain the text of the recycling instructions printed on the packaging (e.g. \"bottle to recycle, cap to discard\"), or can be filled in by users (e.g. \"1 PET plastic bottle to recycle, 1 plastic cap\"). Current resulting packagings data structure (READ only) # The input fields are analyzed and combined to create the \"packagings\" data structure. The structure is an array of packaging components. Each packaging component can have values for different attributes: number: the number of units for the packaging component (e.g. a pack of beers may contain 6 bottles) shape: the general shape of the packaging component (e.g. \"bottle\", \"box\") material: the material of the packaging component quantity: how much product the packaging component contains (e.g. \"25 cl\") recycling: whether the packaging component should be recycled, discarded or reused The \"shape\" and \"material\" fields are taxonomized using the packaging_shapes and packaging_materials taxonomies. How the the resulting packagings data structure is created # Extract attributes that relate to different packaging components # The values for each input field (\"packaging\" tag field and \"packaging_text_[language code]\" packaging information text field) are analyzed 1 to recognize packaging components and their attributes. One product may have multiple \"packaging_text_[language code]\" values in different languages. Only the value for the main product of the language is currently analyzed. For instance, if the \"packaging\" field contains \"Plastic bottle, box, cardboard\", we will use the packaging shapes, materials and recycling taxonomies to create a list of 3 packaging components: {shape:\"en:bottle\", material:\"en:plastic\"}, {shape:\"en:box\"}, {material:\"en:cardboard\"}. And if the \"packaging_text_en\" field contains \"PET bottle to recycle, box to reuse\", we will create 2 more packaging components: {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}, {shape:\"box\", recycling:\"reuse\"}. Merge packaging components # The 3 + 2 = 5 resulting packaging components are then added one by one in the packagings structure. When their attributes are compatible, the packaging units are merged 2 . For instance {shape:\"en:box\"} and {material:\"en:cardboard\"} have non conflicting attributes, so they are merged into {shape:\"en:box\", material:\"en:cardboard\"}. Note that it is possible that this is a mistake, and that the \"box\" and \"cardboard\" tags concern in fact different components. Similarly, as \"en:plastic\" is a parent of \"en:pet-polyethylene-terephthalate\" in the packaging_materials taxonomy, we can merge {shape:\"en:bottle\", material:\"en:plastic\"} with {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"} into {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}. The resulting structure is: packagings: [ { material: \"en:pet-polyethylene-terephthalate\", recycling: \"en:recycle\", shape: \"en:bottle\" }, { recycling: \"en:reuse\", shape: \"en:box\" }, { shape: \"en:container\" } ] Taxonomies # We have created a number of multilingual taxonomies related to packagings: Packaging shapes taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_shapes.txt Packaging materials taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_materials.txt Packaging recycling taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_recycling.txt Preservation methods taxonomy (related) : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/preservation.txt Those taxonomies are used to structure packaging data in Open Food Facts, and to analyze unstructured input data. How we could improve it # Extend the attributes of the packaging components in the \"packagings\" data structure # Weight # We need to add an attribute for the weight of the packaging component. We might need to add different fields to distinguish values that have been entered by users that weight the packaging, versus values provided by the manufacturer, or average values that we have determined from other products, or that we got from external sources. Make the \"packagings\" data structure READ and WRITE # The \"packagings\" data structure is currently a READ only field. We could create an API to make it a READ and WRITE field. For new products, clients (website and apps) could ask users to enter data about all packaging components of the product. For existing products, clients could display the packaging components and let users change them (e.g. adding or removing components, entering values for new attributes, editing attributes to add more precise values (e.g. which type of plastic) etc.). Add a way to indicate that the \"packagings\" data structure contains all the packaging components of the product # We currently have no way to know if the packaging data we have for a product is complete, or if we may be missing some packaging components. We could have a way (e.g. a checkbox) that users could use to indicate all components are accounted for. And we could also do the reverse, and indicate that it is very likely that we are missing some packaging components (e.g. if we have a \"cap\" but no other component to put the cap on). Deprecate the \"packaging\" tags field # We could discard the existing \"packaging\" tags field, and replace it with an API to allow clients to add partial information about packaging components. For instance, if Robotoff detects that the product is in plastic bottle by analyzing a product photo, it could send {shape:\"bottle\", material:\"en:plastic\"} and it would be added / combined with the existing \"packagings\" data. Keep the \"packaging_text_[language code]\" field # It is important to keep this field, as we can display it as-is, use it as input data, and it may contain interesting data that we do not analyze yet. When filled, the values for this field can be analyzed and added to / combined with the \"packagings\" data structure. Similarly to ingredient text analysis, we could keep information about which parts of the text were recognized as attributes of a packaging component, and which parts were not recognized and were therefore ignored. Changing the \"packagings\" value will not change the \"packaging_text_[language code]\" values. Challenges # Incomplete lists of packaging components # Slightly mismatched data from different sources # For a single product, we might get partial packaging data from different sources that we map to similar but distinct shapes, like \"bottle\", \"jar\" and \"jug\". It may be difficult to determine if the data concerns a single packaging component, or different components. Products with packaging changes # Ressources # 2020 project to start structuring packaging data: https://wiki.openfoodfacts.org/Packagings_data_structure parse_packaging_from_text_phrase() function in /lib/ProductOpener/Packagings.pm \u21a9 analyze_and_combine_packaging_data() function in /lib/ProductOpener/Packagings.pm \u21a9","title":"Explanation on packaging data"},{"location":"dev/explain-packaging-data/#explanation-on-packaging-data","text":"This document explains how packaging data is currently added, updated and structured in the Open Food Facts database, and how it could be improved.","title":"Explanation on packaging data"},{"location":"dev/explain-packaging-data/#introduction","text":"","title":"Introduction"},{"location":"dev/explain-packaging-data/#types-of-packaging-data","text":"Food products typically have 1 or more packaging components (e.g. milk may have a bottle and a cap). For each product, we aim to have a comprehensive list of all its packaging components, with detailed information about each packaging component.","title":"Types of packaging data"},{"location":"dev/explain-packaging-data/#data-about-packaging-components","text":"For each packaging component, we want data for different attributes, like its shape (e.g. a bottle) and its size (e.g. plastic). There are many different attributes that can be interesting for specific uses. For instance, researchers in epidemiology are interested in knowing which packaging component is in contact with the food itself, and which one can be put in the microwave oven, so that they can study the long term effects of some plastics on health.","title":"Data about packaging components"},{"location":"dev/explain-packaging-data/#sources-of-packaging-data","text":"We can get packaging data from different sources:","title":"Sources of packaging data"},{"location":"dev/explain-packaging-data/#users","text":"Users of the Open Food Facts website and app, and users of 3rd party apps, can enter packaging data.","title":"Users"},{"location":"dev/explain-packaging-data/#manufacturers","text":"Some manufacturers send product data through GS1, which currently has limited support for packaging information (but this is likely to be improved in the years to come). Some manufacturers send us more detailed packaging data (e.g. recycling instructions) through the Producers Platform. Some manufacturers send us data used to compute the Eco-Score using the Eco-Score spreadsheet template, which has fields like \"Packaging 1\", \"Material 1\", \"Packaging 2\", \"Material 2\" etc.","title":"Manufacturers"},{"location":"dev/explain-packaging-data/#product-photos-and-machine-learning","text":"We can extract logos related to packaging, or parse the text recognized from product photos to recognize packaging information or recycling instructions.","title":"Product photos and machine learning"},{"location":"dev/explain-packaging-data/#how-packaging-data-is-currently-added-updated-and-structured-in-open-food-facts","text":"In Open Food Facts, we currently have a number of input fields related to packaging. The data in those fields is parsed and analyzed to create a structured list of packaging components with specific attributes.","title":"How packaging data is currently added, updated and structured in Open Food Facts"},{"location":"dev/explain-packaging-data/#current-input-fields","text":"","title":"Current input fields"},{"location":"dev/explain-packaging-data/#packaging-tag-field-read-and-write","text":"At the start of Open Food Facts in 2012, we had a \"packaging\" tag field where users could enter comma separated free text entries about the packaging (e.g. \"Plastic\", \"Bag\" or \"Plastic bag\") in different languages. In 2020, we made this field a taxonomized field. As a result, we now store the language used to fill this field, so that we can match its value to the multilingual packaging taxonomy. So \"plastique\" in French will be mapped to the canonical \"en:plastic\" entry.","title":"Packaging tag field (READ and WRITE)"},{"location":"dev/explain-packaging-data/#packaging-information-recycling-instructions-text-field-read-and-write","text":"In 2020, we also added a language specific field (\"packaging_text_[language code]\" e.g. \"packaging_text_en\" for English) to store free text data about the packaging. It can contain the text of the recycling instructions printed on the packaging (e.g. \"bottle to recycle, cap to discard\"), or can be filled in by users (e.g. \"1 PET plastic bottle to recycle, 1 plastic cap\").","title":"Packaging information / recycling instructions text field (READ and WRITE)"},{"location":"dev/explain-packaging-data/#current-resulting-packagings-data-structure-read-only","text":"The input fields are analyzed and combined to create the \"packagings\" data structure. The structure is an array of packaging components. Each packaging component can have values for different attributes: number: the number of units for the packaging component (e.g. a pack of beers may contain 6 bottles) shape: the general shape of the packaging component (e.g. \"bottle\", \"box\") material: the material of the packaging component quantity: how much product the packaging component contains (e.g. \"25 cl\") recycling: whether the packaging component should be recycled, discarded or reused The \"shape\" and \"material\" fields are taxonomized using the packaging_shapes and packaging_materials taxonomies.","title":"Current resulting packagings data structure (READ only)"},{"location":"dev/explain-packaging-data/#how-the-the-resulting-packagings-data-structure-is-created","text":"","title":"How the the resulting packagings data structure is created"},{"location":"dev/explain-packaging-data/#extract-attributes-that-relate-to-different-packaging-components","text":"The values for each input field (\"packaging\" tag field and \"packaging_text_[language code]\" packaging information text field) are analyzed 1 to recognize packaging components and their attributes. One product may have multiple \"packaging_text_[language code]\" values in different languages. Only the value for the main product of the language is currently analyzed. For instance, if the \"packaging\" field contains \"Plastic bottle, box, cardboard\", we will use the packaging shapes, materials and recycling taxonomies to create a list of 3 packaging components: {shape:\"en:bottle\", material:\"en:plastic\"}, {shape:\"en:box\"}, {material:\"en:cardboard\"}. And if the \"packaging_text_en\" field contains \"PET bottle to recycle, box to reuse\", we will create 2 more packaging components: {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}, {shape:\"box\", recycling:\"reuse\"}.","title":"Extract attributes that relate to different packaging components"},{"location":"dev/explain-packaging-data/#merge-packaging-components","text":"The 3 + 2 = 5 resulting packaging components are then added one by one in the packagings structure. When their attributes are compatible, the packaging units are merged 2 . For instance {shape:\"en:box\"} and {material:\"en:cardboard\"} have non conflicting attributes, so they are merged into {shape:\"en:box\", material:\"en:cardboard\"}. Note that it is possible that this is a mistake, and that the \"box\" and \"cardboard\" tags concern in fact different components. Similarly, as \"en:plastic\" is a parent of \"en:pet-polyethylene-terephthalate\" in the packaging_materials taxonomy, we can merge {shape:\"en:bottle\", material:\"en:plastic\"} with {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"} into {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}. The resulting structure is: packagings: [ { material: \"en:pet-polyethylene-terephthalate\", recycling: \"en:recycle\", shape: \"en:bottle\" }, { recycling: \"en:reuse\", shape: \"en:box\" }, { shape: \"en:container\" } ]","title":"Merge packaging components"},{"location":"dev/explain-packaging-data/#taxonomies","text":"We have created a number of multilingual taxonomies related to packagings: Packaging shapes taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_shapes.txt Packaging materials taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_materials.txt Packaging recycling taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_recycling.txt Preservation methods taxonomy (related) : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/preservation.txt Those taxonomies are used to structure packaging data in Open Food Facts, and to analyze unstructured input data.","title":"Taxonomies"},{"location":"dev/explain-packaging-data/#how-we-could-improve-it","text":"","title":"How we could improve it"},{"location":"dev/explain-packaging-data/#extend-the-attributes-of-the-packaging-components-in-the-packagings-data-structure","text":"","title":"Extend the attributes of the packaging components in the \"packagings\" data structure"},{"location":"dev/explain-packaging-data/#weight","text":"We need to add an attribute for the weight of the packaging component. We might need to add different fields to distinguish values that have been entered by users that weight the packaging, versus values provided by the manufacturer, or average values that we have determined from other products, or that we got from external sources.","title":"Weight"},{"location":"dev/explain-packaging-data/#make-the-packagings-data-structure-read-and-write","text":"The \"packagings\" data structure is currently a READ only field. We could create an API to make it a READ and WRITE field. For new products, clients (website and apps) could ask users to enter data about all packaging components of the product. For existing products, clients could display the packaging components and let users change them (e.g. adding or removing components, entering values for new attributes, editing attributes to add more precise values (e.g. which type of plastic) etc.).","title":"Make the \"packagings\" data structure READ and WRITE"},{"location":"dev/explain-packaging-data/#add-a-way-to-indicate-that-the-packagings-data-structure-contains-all-the-packaging-components-of-the-product","text":"We currently have no way to know if the packaging data we have for a product is complete, or if we may be missing some packaging components. We could have a way (e.g. a checkbox) that users could use to indicate all components are accounted for. And we could also do the reverse, and indicate that it is very likely that we are missing some packaging components (e.g. if we have a \"cap\" but no other component to put the cap on).","title":"Add a way to indicate that the \"packagings\" data structure contains all the packaging components of the product"},{"location":"dev/explain-packaging-data/#deprecate-the-packaging-tags-field","text":"We could discard the existing \"packaging\" tags field, and replace it with an API to allow clients to add partial information about packaging components. For instance, if Robotoff detects that the product is in plastic bottle by analyzing a product photo, it could send {shape:\"bottle\", material:\"en:plastic\"} and it would be added / combined with the existing \"packagings\" data.","title":"Deprecate the \"packaging\" tags field"},{"location":"dev/explain-packaging-data/#keep-the-packaging_text_language-code-field","text":"It is important to keep this field, as we can display it as-is, use it as input data, and it may contain interesting data that we do not analyze yet. When filled, the values for this field can be analyzed and added to / combined with the \"packagings\" data structure. Similarly to ingredient text analysis, we could keep information about which parts of the text were recognized as attributes of a packaging component, and which parts were not recognized and were therefore ignored. Changing the \"packagings\" value will not change the \"packaging_text_[language code]\" values.","title":"Keep the \"packaging_text_[language code]\" field"},{"location":"dev/explain-packaging-data/#challenges","text":"","title":"Challenges"},{"location":"dev/explain-packaging-data/#incomplete-lists-of-packaging-components","text":"","title":"Incomplete lists of packaging components"},{"location":"dev/explain-packaging-data/#slightly-mismatched-data-from-different-sources","text":"For a single product, we might get partial packaging data from different sources that we map to similar but distinct shapes, like \"bottle\", \"jar\" and \"jug\". It may be difficult to determine if the data concerns a single packaging component, or different components.","title":"Slightly mismatched data from different sources"},{"location":"dev/explain-packaging-data/#products-with-packaging-changes","text":"","title":"Products with packaging changes"},{"location":"dev/explain-packaging-data/#ressources","text":"2020 project to start structuring packaging data: https://wiki.openfoodfacts.org/Packagings_data_structure parse_packaging_from_text_phrase() function in /lib/ProductOpener/Packagings.pm \u21a9 analyze_and_combine_packaging_data() function in /lib/ProductOpener/Packagings.pm \u21a9","title":"Ressources"},{"location":"dev/explain-pro-dev-setup/","text":"Explanation on Docker Setup of pro platform for development # This explains how we setup docker file for pro platform development. For explanations on how to use it, see: how-to-guides/pro-development off is the public facing application (world.openfoodfacts.org) off-pro is the producers platform (world.pro.openfoodfacts.org) When we work on the pro platform for development we want: * off containers to talk between each other, and have their own volumes * off-pro containers to talk between each other, and, generally, have their own volumes * minion and backend from both app access to the same postgres database (which stores tasks queues) * off and off-pro backends / minion needs to share some volumes : orgs, users ands some files living in podata Still we would like to avoid having different clone of the repository, but we can isolate projects thanks to COMPOSE_PROJECT_NAME , which will prefix containers names, volumes and default network, thus isolate each projects. This is achieved by sourcing the .env-pro which setup some environment variables that will superseed the .env variables. The main one being setting COMPOSE_PROJECT_NAME and PRODUCERS_PLATFORM , but also other like MINION_QUEUE . On volume side, we will simply give hard-coded names to volumes that should be shared between off and pro platform, thus they will be shared. Ideally we should not have to share single files but this is a work in progress, we will live without it as a first approx. To satisfy the access to the same database, we will use postgres database from off as the common database. In order to achieve that: * we use profiles, so we won't start postgres in pro docker-compose * we connect postgres , backend and minion services to a shared network, called minion_db Fortunately this works, but note that there is a pitfall: on minion_db network both backend services ( off and off-pro ) will respond to same name. For the moment it is not a problem for we don't need to communicate directly between instances. If it was, we would have to define custom aliases for those services on the minion_db network. network OFF network PRO network po_default containers minion_db containers po_pro_default | | | +------postgres------------+ | | | | | | | +-----backend--------------+ | | +----------backend-------------+ | | | +------minion--------------+ | | +----------minion--------------+ | | | | | | +------frontend | frontend------------+ +------mongodb | mongodb-------------+ | | |","title":"Explanation on Docker Setup of pro platform for development"},{"location":"dev/explain-pro-dev-setup/#explanation-on-docker-setup-of-pro-platform-for-development","text":"This explains how we setup docker file for pro platform development. For explanations on how to use it, see: how-to-guides/pro-development off is the public facing application (world.openfoodfacts.org) off-pro is the producers platform (world.pro.openfoodfacts.org) When we work on the pro platform for development we want: * off containers to talk between each other, and have their own volumes * off-pro containers to talk between each other, and, generally, have their own volumes * minion and backend from both app access to the same postgres database (which stores tasks queues) * off and off-pro backends / minion needs to share some volumes : orgs, users ands some files living in podata Still we would like to avoid having different clone of the repository, but we can isolate projects thanks to COMPOSE_PROJECT_NAME , which will prefix containers names, volumes and default network, thus isolate each projects. This is achieved by sourcing the .env-pro which setup some environment variables that will superseed the .env variables. The main one being setting COMPOSE_PROJECT_NAME and PRODUCERS_PLATFORM , but also other like MINION_QUEUE . On volume side, we will simply give hard-coded names to volumes that should be shared between off and pro platform, thus they will be shared. Ideally we should not have to share single files but this is a work in progress, we will live without it as a first approx. To satisfy the access to the same database, we will use postgres database from off as the common database. In order to achieve that: * we use profiles, so we won't start postgres in pro docker-compose * we connect postgres , backend and minion services to a shared network, called minion_db Fortunately this works, but note that there is a pitfall: on minion_db network both backend services ( off and off-pro ) will respond to same name. For the moment it is not a problem for we don't need to communicate directly between instances. If it was, we would have to define custom aliases for those services on the minion_db network. network OFF network PRO network po_default containers minion_db containers po_pro_default | | | +------postgres------------+ | | | | | | | +-----backend--------------+ | | +----------backend-------------+ | | | +------minion--------------+ | | +----------minion--------------+ | | | | | | +------frontend | frontend------------+ +------mongodb | mongodb-------------+ | | |","title":"Explanation on Docker Setup of pro platform for development"},{"location":"dev/explain-taxonomy-build-cache/","text":"Explanation on Taxonomy Build Cache # Taxonomies have a significant impact on OFF processing and automated test results so need to be rebuilt before running any tests. However, this process takes some time, so the built taxonomy files are cached in a GitHub repository so that they only need to be rebuilt when there is a genuine change. How it works # A hash is calculated for all of the source files used to build a particular taxonomy and GitHub is then checked to see if a cache already exists for that hash. If no cached build is found then the taxonomy is rebuilt and cached locally. If the GITHUB_TOKEN environemnt variable is set then the cached build is also uploaded to the https://github.com/openfoodfacts/openfoodfacts-build-cache repository. Note that no token is required to download previous cached builds from the repo. Obtaining a token # The GITHUB_TOKEN is a personal access token, created here: https://github.com/settings/tokens. Only the public_repo scope is needed. Considerations # In maintianing this code be aware of the following complications... Circular Dependencies # There is a cicular dependency between taxonomies, languages and foods. The foods library is used to create the source for the nutrient_levels taxonomy, which uses transalations from languages. However, languages depends on the languages taxonomy... This is currently resolved by building the taxonomy on the fly if it is requested but not currently built. Taxonomy Dependencies # Some taxonomies perform lookups on others, e.g. additives_classes are referenced by additives, so the referenced taxonomy needs to be built first. The build order is determined in the Config_off.pm file.","title":"Explanation on Taxonomy Build Cache"},{"location":"dev/explain-taxonomy-build-cache/#explanation-on-taxonomy-build-cache","text":"Taxonomies have a significant impact on OFF processing and automated test results so need to be rebuilt before running any tests. However, this process takes some time, so the built taxonomy files are cached in a GitHub repository so that they only need to be rebuilt when there is a genuine change.","title":"Explanation on Taxonomy Build Cache"},{"location":"dev/explain-taxonomy-build-cache/#how-it-works","text":"A hash is calculated for all of the source files used to build a particular taxonomy and GitHub is then checked to see if a cache already exists for that hash. If no cached build is found then the taxonomy is rebuilt and cached locally. If the GITHUB_TOKEN environemnt variable is set then the cached build is also uploaded to the https://github.com/openfoodfacts/openfoodfacts-build-cache repository. Note that no token is required to download previous cached builds from the repo.","title":"How it works"},{"location":"dev/explain-taxonomy-build-cache/#obtaining-a-token","text":"The GITHUB_TOKEN is a personal access token, created here: https://github.com/settings/tokens. Only the public_repo scope is needed.","title":"Obtaining a token"},{"location":"dev/explain-taxonomy-build-cache/#considerations","text":"In maintianing this code be aware of the following complications...","title":"Considerations"},{"location":"dev/explain-taxonomy-build-cache/#circular-dependencies","text":"There is a cicular dependency between taxonomies, languages and foods. The foods library is used to create the source for the nutrient_levels taxonomy, which uses transalations from languages. However, languages depends on the languages taxonomy... This is currently resolved by building the taxonomy on the fly if it is requested but not currently built.","title":"Circular Dependencies"},{"location":"dev/explain-taxonomy-build-cache/#taxonomy-dependencies","text":"Some taxonomies perform lookups on others, e.g. additives_classes are referenced by additives, so the referenced taxonomy needs to be built first. The build order is determined in the Config_off.pm file.","title":"Taxonomy Dependencies"},{"location":"dev/how-to-deploy/","text":"How to deploy to Prod environment # Note: prod deployment is very manual and not automated yet. Login to the off1 server, as the \"off\" user cd /home/off/openfoodfacts-server Check that you are on the main branch git pull Copy changed files (don't copy everything, in particular not the lang directory that is being moved to the openfoodfacts-web repository) e.g. cp cgi scripts lib po taxonomies templates /srv/off/ cd /srv/off export NPM_CONFIG_PREFIX=~/.npm-global npm install npm run build cd /srv/off/cgi export PERL5LIB=. ./build_lang.pl as the root user: systemctl stop apache2@off systemctl start apache2@off systemctl stop minion-off systemctl start minion-off","title":"How to deploy to Prod environment"},{"location":"dev/how-to-deploy/#how-to-deploy-to-prod-environment","text":"Note: prod deployment is very manual and not automated yet. Login to the off1 server, as the \"off\" user cd /home/off/openfoodfacts-server Check that you are on the main branch git pull Copy changed files (don't copy everything, in particular not the lang directory that is being moved to the openfoodfacts-web repository) e.g. cp cgi scripts lib po taxonomies templates /srv/off/ cd /srv/off export NPM_CONFIG_PREFIX=~/.npm-global npm install npm run build cd /srv/off/cgi export PERL5LIB=. ./build_lang.pl as the root user: systemctl stop apache2@off systemctl start apache2@off systemctl stop minion-off systemctl start minion-off","title":"How to deploy to Prod environment"},{"location":"dev/how-to-develop-producer-platform/","text":"How to develop on the producers platform # Here is how to develop for the producers platform using docker. It suppose you already have setup docker for dev . You should have two kind of shell: - the shell for openfoodfacts - the shell for openfoodfacts-pro, this is a shell where you have source the setenv-pro.sh , that is you run . setenv-pro.sh . Your prompt, should now contains a (pro) to recall you you are in producers environment. (this simply sets some environment variables that will overides the one in .env) To develop, on producers plateform, you can then us a shell for openfoodfacts-pro and simply do a make dev and everything as usual. If you need to work on product import/export, or interacting with public platform, you have to start postgres and the minion on off side. That is, in a non pro shell, run docker-compose up postgres minion mongodb . Note that the setup does not currently support running the http server for both public and pro platform at the same time. So as you need the public platform: - in your pro shell , run a docker-compose stop backend - in your non pro shell , run a docker-compose up backend Now openfoodfacts.localhost is the public database. Of course, do this inside-out to access the pro http server. Note that if you use direnv , it should be fine, if you did not redefine variables set by setenv-pro.sh . An explanation of the setup can be found at explain-pro-dev-setup.md If you want to see state of tasks, you can run: docker-compose exec minion /opt/product-opener/scripts/minion_producers.pl minion job (add --help to see all options), or refer to https://docs.mojolicious.org/Minion/Command/minion/job You may also inspect database by running: docker-compose exec postgres psql -U productopener -W minion (password is given by POSTGRES_PASSWORD in .env and defaults to productopener ) Inspecting table minion, should help.","title":"How to develop on the producers platform"},{"location":"dev/how-to-develop-producer-platform/#how-to-develop-on-the-producers-platform","text":"Here is how to develop for the producers platform using docker. It suppose you already have setup docker for dev . You should have two kind of shell: - the shell for openfoodfacts - the shell for openfoodfacts-pro, this is a shell where you have source the setenv-pro.sh , that is you run . setenv-pro.sh . Your prompt, should now contains a (pro) to recall you you are in producers environment. (this simply sets some environment variables that will overides the one in .env) To develop, on producers plateform, you can then us a shell for openfoodfacts-pro and simply do a make dev and everything as usual. If you need to work on product import/export, or interacting with public platform, you have to start postgres and the minion on off side. That is, in a non pro shell, run docker-compose up postgres minion mongodb . Note that the setup does not currently support running the http server for both public and pro platform at the same time. So as you need the public platform: - in your pro shell , run a docker-compose stop backend - in your non pro shell , run a docker-compose up backend Now openfoodfacts.localhost is the public database. Of course, do this inside-out to access the pro http server. Note that if you use direnv , it should be fine, if you did not redefine variables set by setenv-pro.sh . An explanation of the setup can be found at explain-pro-dev-setup.md If you want to see state of tasks, you can run: docker-compose exec minion /opt/product-opener/scripts/minion_producers.pl minion job (add --help to see all options), or refer to https://docs.mojolicious.org/Minion/Command/minion/job You may also inspect database by running: docker-compose exec postgres psql -U productopener -W minion (password is given by POSTGRES_PASSWORD in .env and defaults to productopener ) Inspecting table minion, should help.","title":"How to develop on the producers platform"},{"location":"dev/how-to-develop-using-docker/","text":"How to use Docker to Develop - a guide # This guide is for developers and newcomers to help them debug and explore Docker. This page describes how to test and debug your changes once you have set up the project, Product Opener with Docker using dev environment quick start guide . Checking logs # Tail Docker Compose logs # make log You will get logs from nginx, mongodb, postgres, etc. Tail other logs # Most logs from perl are not (yet ?) displayed on the docker logs, but are instead available in specific directories. To see them use: make tail It will tail -f all the files present in the logs/ directory: apache2/error.log apache2/log4perl.log apache2/modperl_error.log apache2/other_vhosts_access.log nginx/access.log nginx/error.log You can also simply run: tail -f <FILEPATH> to check a specific log. One of the most important is log4perl.log . Increasing log verbosity # By default, the log4perl configuration conf/log.conf matches production settings. You can tweak that file with your own dev configuration settings and run make restart to reload the changes. A setting useful for local environments is to set TRACE log level: log4perl.rootLogger=TRACE, LOGFILE Opening a shell in a Docker container # Run the following to open a bash shell within the backend container: docker-compose exec backend bash You should see root@<CONTAINER_ID>:/# (opened root shell): you are now within the Docker container and can begin typing some commands ! Checking permissions # Navigate to the directory the specific directory and run ls -lrt It will list all the directories and their permission status. Creating directory # Navigate to your specific directory using cd command and run mkdir directory-name Running minion jobs # Minion is a high-performance job queue for Perl. Minion is used in openfoodfacts-server for time-consuming import and export tasks. These tasks are processed and queued using the minion jobs queue. Therefore, they are called minion jobs. Go to /opt/product-opener/scripts and run ./minion_producers.pl minion job The above command will show the status of minion jobs. Run the following command to launch the minion jobs. ./minion_producers.pl minion worker -m production -q pro.openfoodfacts.org Restarting Apache # Sometimes restarting the whole backend container is overkill, and you can just restart Apache from inside the container: apache2ctl -k restart Exiting the container # Use exit to exit the container. Making code changes # In the dev environment, any code change to the local directory will be written on the container. That said, some code changes require a restart of the backend container, or rebuilding the NPM assets. Getting away from make up # make up is a good command for starters, but it's not the right one to use if you develop on a daily bases, because it maybe slow, as it does a full rebuild, which, in dev mode, should only be necessary in a few cases. On a daily bases you could better run those: docker-compose up to start and monitor the stack. docker-compose restart backend to account for a code change in a .pm file (cgi pl files do not need a restart) docker-compose stop to stop them all If some important file changed (like Dockerfile or cpanfile, etc.), or in case of doubt, you can run docker-compose build (or maybe it's a good time to use make up once) You should explore the docker-compose commands most are useful! Live reload # To automate the live reload on code changes, you can install the Python package when-changed : pip3 install when-changed when-changed -r docker/ docker-compose.yml .env -c \"make restart\" # restart backend container on compose changes when-changed -r lib/ -r docker/ docker-compose.yml -c \"docker-compose backend restart\" # restart Apache on code changes when-changed -r html/ Dockerfile Dockerfile.frontend package.json -c \"make up\" # rebuild containers on asset or Dockerfile changes An alternative to when-changed is inotifywait . Run queries on MongoDB database # docker-compose exec mongodb mongo The above command will open a MongoDB shell, allowing you to use all the mongo commands to interact with the database: show dbs use off db.products.count() db.products.find({_id: \"5053990155354\"}) db.products.deleteOne({_id: \"5053990155354\"}) See the mongo shell docs for more commands. Adding environment variables # If you need some value to be configurable, it is best to set is as an environment variable. To add a new environment variable TEST : In .env file, add TEST=test_val [local]. In .github/workflows/container-deploy.yml , add echo \"TEST=${{ secrets.TEST }}\" >> .env to the \"Set environment variables\" build step [remote]. Also add the corresponding GitHub secret TEST=test_val . In docker-compose.yml file, add it under the backend > environment section. In conf/apache.conf file, add PerlPassEnv TEST . In lib/Config2.pm , add $test = $ENV{TEST}; . Also add $test to the EXPORT_OK list at the top of the file to avoid a compilation error. The call stack goes like this: make up > docker-compose > loads .env > pass env variables to the backend container > pass to mod_perl > initialized in Config2.pm . Managing multiple deployments # To juggle between multiple local deployments (e.g: to run different flavors of Open Food Facts on the same host), there are different possible strategies. a set env script # Docker-compose takes it settings from, in order of priority: * the environment * the .env file So one strategy to have a different instance, can be to keep same .env file, but super-seed some env variables to tweak the configuration. This is a good strategy for the pro plateform. For this case we have a setenv-pro.sh script. To use it, open a terminal, where you want to be in pro environment and simply use: . setenv-pro.sh then you can use whatever docker-compose command. Note: This terminal will remain in pro mode until you end its session. See also Developing on the producers platform different .env file # This strategy might be the right one if your settings differs a lot. You will need: Multiple .env files (one per deployment), such as: .env.off : configuration for Open Food Facts dev env. .env.off-pro : configuration for Open Food Facts Producer's Platform dev env. .env.obf : configuration for Open Beauty Facts dev env. .env.opff : configuration for Open Ped Food Facts dev env. COMPOSE_PROJECT_NAME set to different values in each .env file, so that container names across deployments are unique. FRONTEND_PORT and MONGODB_PORT set to different values in each .env file, so that frontend containers don't port-conflict with each other. To switch between configurations, set ENV_FILE before running make commands, (or docker-compose command): ENV_FILE=.env.off-pro make up # starts the OFF Producer's Platform containers. ENV_FILE=.env.obf make up # starts the OBF containers. ENV_FILE=.env.opff make up # starts the OPFF containers. or export it to keep it for a while: export ENV_FILE=.env.off # going to work on OFF for a while make up make restart make down make log A good strategy is to have multiple terminals open, one for each deployment: off [Terminal 1]: export ENV_FILE=.env.off make up off-pro [Terminal 2]: export ENV_FILE=.env.off-pro make up obf [Terminal 3]: export ENV_FILE=.env.obf make up opff [Terminal 3]: export ENV_FILE=.env.opff make up Note: the above case of 4 deployments is a bit ambitious , since ProductOpener's backend container takes about ~6GB of RAM to run, meaning that the above 4 deployments would require a total of 24GB of RAM available.","title":"How to use Docker to Develop - a guide"},{"location":"dev/how-to-develop-using-docker/#how-to-use-docker-to-develop-a-guide","text":"This guide is for developers and newcomers to help them debug and explore Docker. This page describes how to test and debug your changes once you have set up the project, Product Opener with Docker using dev environment quick start guide .","title":"How to use Docker to Develop - a guide"},{"location":"dev/how-to-develop-using-docker/#checking-logs","text":"","title":"Checking logs"},{"location":"dev/how-to-develop-using-docker/#tail-docker-compose-logs","text":"make log You will get logs from nginx, mongodb, postgres, etc.","title":"Tail Docker Compose logs"},{"location":"dev/how-to-develop-using-docker/#tail-other-logs","text":"Most logs from perl are not (yet ?) displayed on the docker logs, but are instead available in specific directories. To see them use: make tail It will tail -f all the files present in the logs/ directory: apache2/error.log apache2/log4perl.log apache2/modperl_error.log apache2/other_vhosts_access.log nginx/access.log nginx/error.log You can also simply run: tail -f <FILEPATH> to check a specific log. One of the most important is log4perl.log .","title":"Tail other logs"},{"location":"dev/how-to-develop-using-docker/#increasing-log-verbosity","text":"By default, the log4perl configuration conf/log.conf matches production settings. You can tweak that file with your own dev configuration settings and run make restart to reload the changes. A setting useful for local environments is to set TRACE log level: log4perl.rootLogger=TRACE, LOGFILE","title":"Increasing log verbosity"},{"location":"dev/how-to-develop-using-docker/#opening-a-shell-in-a-docker-container","text":"Run the following to open a bash shell within the backend container: docker-compose exec backend bash You should see root@<CONTAINER_ID>:/# (opened root shell): you are now within the Docker container and can begin typing some commands !","title":"Opening a shell in a Docker container"},{"location":"dev/how-to-develop-using-docker/#checking-permissions","text":"Navigate to the directory the specific directory and run ls -lrt It will list all the directories and their permission status.","title":"Checking permissions"},{"location":"dev/how-to-develop-using-docker/#creating-directory","text":"Navigate to your specific directory using cd command and run mkdir directory-name","title":"Creating directory"},{"location":"dev/how-to-develop-using-docker/#running-minion-jobs","text":"Minion is a high-performance job queue for Perl. Minion is used in openfoodfacts-server for time-consuming import and export tasks. These tasks are processed and queued using the minion jobs queue. Therefore, they are called minion jobs. Go to /opt/product-opener/scripts and run ./minion_producers.pl minion job The above command will show the status of minion jobs. Run the following command to launch the minion jobs. ./minion_producers.pl minion worker -m production -q pro.openfoodfacts.org","title":"Running minion jobs"},{"location":"dev/how-to-develop-using-docker/#restarting-apache","text":"Sometimes restarting the whole backend container is overkill, and you can just restart Apache from inside the container: apache2ctl -k restart","title":"Restarting Apache"},{"location":"dev/how-to-develop-using-docker/#exiting-the-container","text":"Use exit to exit the container.","title":"Exiting the container"},{"location":"dev/how-to-develop-using-docker/#making-code-changes","text":"In the dev environment, any code change to the local directory will be written on the container. That said, some code changes require a restart of the backend container, or rebuilding the NPM assets.","title":"Making code changes"},{"location":"dev/how-to-develop-using-docker/#getting-away-from-make-up","text":"make up is a good command for starters, but it's not the right one to use if you develop on a daily bases, because it maybe slow, as it does a full rebuild, which, in dev mode, should only be necessary in a few cases. On a daily bases you could better run those: docker-compose up to start and monitor the stack. docker-compose restart backend to account for a code change in a .pm file (cgi pl files do not need a restart) docker-compose stop to stop them all If some important file changed (like Dockerfile or cpanfile, etc.), or in case of doubt, you can run docker-compose build (or maybe it's a good time to use make up once) You should explore the docker-compose commands most are useful!","title":"Getting away from make up"},{"location":"dev/how-to-develop-using-docker/#live-reload","text":"To automate the live reload on code changes, you can install the Python package when-changed : pip3 install when-changed when-changed -r docker/ docker-compose.yml .env -c \"make restart\" # restart backend container on compose changes when-changed -r lib/ -r docker/ docker-compose.yml -c \"docker-compose backend restart\" # restart Apache on code changes when-changed -r html/ Dockerfile Dockerfile.frontend package.json -c \"make up\" # rebuild containers on asset or Dockerfile changes An alternative to when-changed is inotifywait .","title":"Live reload"},{"location":"dev/how-to-develop-using-docker/#run-queries-on-mongodb-database","text":"docker-compose exec mongodb mongo The above command will open a MongoDB shell, allowing you to use all the mongo commands to interact with the database: show dbs use off db.products.count() db.products.find({_id: \"5053990155354\"}) db.products.deleteOne({_id: \"5053990155354\"}) See the mongo shell docs for more commands.","title":"Run queries on MongoDB database"},{"location":"dev/how-to-develop-using-docker/#adding-environment-variables","text":"If you need some value to be configurable, it is best to set is as an environment variable. To add a new environment variable TEST : In .env file, add TEST=test_val [local]. In .github/workflows/container-deploy.yml , add echo \"TEST=${{ secrets.TEST }}\" >> .env to the \"Set environment variables\" build step [remote]. Also add the corresponding GitHub secret TEST=test_val . In docker-compose.yml file, add it under the backend > environment section. In conf/apache.conf file, add PerlPassEnv TEST . In lib/Config2.pm , add $test = $ENV{TEST}; . Also add $test to the EXPORT_OK list at the top of the file to avoid a compilation error. The call stack goes like this: make up > docker-compose > loads .env > pass env variables to the backend container > pass to mod_perl > initialized in Config2.pm .","title":"Adding environment variables"},{"location":"dev/how-to-develop-using-docker/#managing-multiple-deployments","text":"To juggle between multiple local deployments (e.g: to run different flavors of Open Food Facts on the same host), there are different possible strategies.","title":"Managing multiple deployments"},{"location":"dev/how-to-develop-using-docker/#a-set-env-script","text":"Docker-compose takes it settings from, in order of priority: * the environment * the .env file So one strategy to have a different instance, can be to keep same .env file, but super-seed some env variables to tweak the configuration. This is a good strategy for the pro plateform. For this case we have a setenv-pro.sh script. To use it, open a terminal, where you want to be in pro environment and simply use: . setenv-pro.sh then you can use whatever docker-compose command. Note: This terminal will remain in pro mode until you end its session. See also Developing on the producers platform","title":"a set env script"},{"location":"dev/how-to-develop-using-docker/#different-env-file","text":"This strategy might be the right one if your settings differs a lot. You will need: Multiple .env files (one per deployment), such as: .env.off : configuration for Open Food Facts dev env. .env.off-pro : configuration for Open Food Facts Producer's Platform dev env. .env.obf : configuration for Open Beauty Facts dev env. .env.opff : configuration for Open Ped Food Facts dev env. COMPOSE_PROJECT_NAME set to different values in each .env file, so that container names across deployments are unique. FRONTEND_PORT and MONGODB_PORT set to different values in each .env file, so that frontend containers don't port-conflict with each other. To switch between configurations, set ENV_FILE before running make commands, (or docker-compose command): ENV_FILE=.env.off-pro make up # starts the OFF Producer's Platform containers. ENV_FILE=.env.obf make up # starts the OBF containers. ENV_FILE=.env.opff make up # starts the OPFF containers. or export it to keep it for a while: export ENV_FILE=.env.off # going to work on OFF for a while make up make restart make down make log A good strategy is to have multiple terminals open, one for each deployment: off [Terminal 1]: export ENV_FILE=.env.off make up off-pro [Terminal 2]: export ENV_FILE=.env.off-pro make up obf [Terminal 3]: export ENV_FILE=.env.obf make up opff [Terminal 3]: export ENV_FILE=.env.opff make up Note: the above case of 4 deployments is a bit ambitious , since ProductOpener's backend container takes about ~6GB of RAM to run, meaning that the above 4 deployments would require a total of 24GB of RAM available.","title":"different .env file"},{"location":"dev/how-to-quick-start-guide/","text":"How to setup the Dev environment (quick start guide) # This guide will allow you to rapidly build a ready-to-use development environment for Product Opener running in Docker. As an alternative to setting up your environment locally, follow the Gitpod how-to guide to instantly provision a ready-to-code development environment in the cloud. First setup time estimate is ~10min with the following specs: * 8 GB of RAM dedicated to Docker client * 6 cores dedicated to Docker client * 12 MB/s internet speed 1. Prerequisites # Docker is the easiest way to install the Open Food Facts server, play with it, and even modify the code. Docker provides an isolated environment, very close to a Virtual Machine. This environment contains everything required to launch the Open Food Facts server. There is no need to install Perl, Perl modules, Nginx, nor Apache separately. Installation steps: - Install Docker CE If you run e.g. Debian, don't forget to add your user to the docker group! - Install Docker Compose - Enable command-line completion Windows Prerequisites # When running with Windows, install Docker Desktop which will cover all of the above. The Make tasks use a number of Linux commands, such as rm and nproc, so it is recommeded to run Make commands from the Git Bash shell. In addition, the following need to be installed and included in the PATH: Make for Windows wget for windows (In order to download the full product database). The process of cloning the repository will create a number of symbolic links which require specific permissions under Windows. In order to do this you can use any one of these alternatives: Use an Administrative command prompt for all Git commands Completely disable UAC Specifically grant the Create symbolic links permission to your user 2. Clone the repository from GitHub # You must have a GitHub account if you want to contribute to Open Food Facts development, but it\u2019s not required if you just want to see how it works. Be aware Open Food Facts server takes more than 1.3 GB (2019/11). Choose your prefered way to clone, either: On Windows: # If you are running Docker on Windows, please use the following git clone command: git clone -c core.symlinks=true https://github.com/openfoodfacts/openfoodfacts-server.git or git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git On other systems: # git clone git@github.com:openfoodfacts/openfoodfacts-server.git or git clone https://github.com/openfoodfacts/openfoodfacts-server.git Go to the cloned directory: cd openfoodfacts-server/ 3. [Optional] Review Product Opener's environment # Note: you can skip this step for the first setup since the default .env in the repo contains all the default values required to get started. Before running the docker-compose deployment, you can review and configure Product Opener's environment ( .env file). The .env file contains ProductOpener default settings: | Field | Description | | ----------------------------------------------------------------- | --- | | PRODUCT_OPENER_DOMAIN | Can be set to different values based on which OFF flavor is run.| | PRODUCT_OPENER_PORT | can be modified to run NGINX on a different port. Useful when running multiple OFF flavors on different ports on the same host. Default port: 80 .| | PRODUCT_OPENER_FLAVOR | Can be modified to run different flavors of OpenFoodFacts, amongst openfoodfacts (default), openbeautyfacts , openpetfoodfacts , openproductsfacts .| | PRODUCT_OPENER_FLAVOR_SHORT | can be modified to run different flavors of OpenFoodFacts, amongst off (default), obf , oppf , opf .| | PRODUCERS_PLATFORM | can be set to 1 to build / run the producer platform .| | ROBOTOFF_URL | can be set to connect with a Robotoff instance .| | REDIS_URL | can be set to connect with a Redis instance for populating the search index .| | GOOGLE_CLOUD_VISION_API_KEY | can be set to enable OCR using Google Cloud Vision .| | CROWDIN_PROJECT_IDENTIFIER and CROWDIN_PROJECT_KEY | can be set to run translations .| | GEOLITE2_PATH , GEOLITE2_ACCOUNT_ID and GEOLITE2_LICENSE_KEY | can be set to enable Geolite2 .| | TAG | Is set to latest by default, but you can specify any Docker Hub tag for the frontend / backend images. Note that this is useful only if you use pre-built images from the Docker Hub ( docker/prod.yml override); the default dev setup ( docker/dev.yml ) builds images locally| The .env file also contains some useful Docker Compose variables: * COMPOSE_PROJECT_NAME is the compose project name that sets the prefix to every container name . Do not update this unless you know what you're doing. * COMPOSE_FILE is the ; -separated list of Docker compose files that are included in the deployment: * For a development -like environment, set it to docker-compose.yml;docker/dev.yml (default) * For a production -like environment, set it to docker-compose.yml;docker/prod.yml;docker/mongodb.yml * For more features, you can add: * docker/admin-uis.yml : add the Admin UIS container * docker/geolite2.yml : add the Geolite2 container * docker/perldb.yml : add the Perl debugger container * COMPOSE_SEPARATOR is the separator used for COMPOSE_FILE . Note: Instead of modifying .env (with the risk commit it inadvertently), You can also set needed variables in your shell, they will override .env values. Consider creating a .envrc file that you source each time you need to work on the project. On linux and macOS, you can automatically do it if you use direnv . 4. Build your dev environment # From the repository root, run: make dev Note: If you are using Windows, you may encounter issues regarding this command. Take a look at the Troubleshooting section further in this tutorial. Note: If docker complains about ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network It can be solved by adding {\"base\": \"172.80.0.0/16\",\"size\": 24}, {\"base\": \"172.90.0.0/16\",\"size\": 24} to default-address-pools in /etc/docker/daemon.json and then restarting the docker daemon. Credits to https://theorangeone.net/posts/increase-docker-ip-space/ for this solution. The command will run 2 subcommands: * make up : Build and run containers from the local directory and bind local code files, so that you do not have to rebuild everytime. * make import_sample_data : Load sample data into mongodb container (~100 products). Notes: The first build can take between 10 and 30 minutes depending on your machine and internet connection (broadband connection heavily recommended, as this will download Docker base images, install Debian and Perl modules in preparation of the final container image). You might not immediately see the test products: create an account, login, and they should appear. For a full description of available make targets, see docker/README.md Hosts file: Since the default PRODUCT_OPENER_DOMAIN in the .env file is set to openfoodfacts.localhost , add the following to your hosts file (Windows: C:\\Windows\\System32\\drivers\\etc\\hosts ; Linux/MacOSX: /etc/hosts ): 127.0.0.1 world.openfoodfacts.localhost fr.openfoodfacts.localhost static.openfoodfacts.localhost ssl-api.openfoodfacts.localhost fr-en.openfoodfacts.localhost You're done ! Check http://openfoodfacts.localhost/ ! Going further # To learn more about developing with Docker, see the Docker developer's guide . To have all site page on your dev instance, see Using pages from openfoodfacts-web Using Repl offers you a way to play with perl. Specific notes are provide on applying AGRIBALYSE updates to support the Ecoscore calculation. Visual Studio Code # WARNING : for now this is deprecated, some work needs to be done. This repository comes with a configuration for Visual Studio Code (VS Code) development containers (devcontainer) . This enables some Perl support in VS Code without the need to install the correct Perl version and modules on your local machine. To use the devcontainer, install prerequisites , clone the repository from GitHub , and (optionally) review Product Opener's environment . Additionally, install Visual Studio Code . VS Code will automatically recommend some extensions, but if you don't want to install all of them, please do install Remote - Containers manually. You can then use the extension command Remote-Containers: Reopen Folder in Container , which will automatically build the container and start the services. No need to use make ! Troubleshooting # make dev error: make: command not found # When running make dev : bash: make: command not found Solution: Click the Windows button, then type \u201cenvironment properties\u201d into the search bar and hit Enter. Click Environment Variables, then under System variables choose Path and click Edit. Click New and insert C:\\Program Files (x86)\\GnuWin32\\bin, then save the changes. Open a new terminal and test that the command works. (see Make Windows for more) make dev error: [build_lang] Error 2 - Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto # When running make dev : <h1>Software error:</h1> <pre>Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. </p> [Tue Apr 5 19:36:40 2022] build_lang.pl: Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. [Tue Apr 5 19:36:40 2022] build_lang.pl: Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. [Tue Apr 5 19:36:40 2022] build_lang.pl: BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. make: *** [build_lang] Error 2 Solution: Project needs Symlinks to be enabled. traces.result.sto is a symlink to allergens.result.sto You have to enable the 'Developer Mode' in order to use the symlinks. To enable Developer Mode: on windows 10: Settings > Update & Security > 'For developers' \u2026 on windows 11: Settings > Privacy & Security > 'For developers' \u2026 and turn on the toggle for Developer Mode . On Windows systems, the git repository needs to be cloned with symlinks enabled. You need to remove current directory where you clone the project, and clone the project again, using right options: git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git 'rm' is not recognized as an internal or external command # When running make import_prod_data or some other commands. Solution: Use the Git Bash shell to run the make commands in windows so that programs like nproc and rm are found. System cannot find wget # When running make import_prod_data . process_begin: CreateProcess(NULL, wget --no-verbose https://static.openfoodfacts.org/data/openfoodfacts-mongodbdump.tar.gz, ...) failed. make (e=2): The system cannot find the file specified. You need to install wget for windows . The referenced version is able to use the Windows Certificate Store, whereas the standard gnuwin32 version will give errors about not being able to verify the server certificate. make: *** [Makefile:154: import_sample_data] Error 22 # When running make import_sample_data <hl>Software error:</h1> <pre>MongoDB: :SelectionError: No writable server available. MongoDB server status: Topology type: Single; Member status: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution ) </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. <p> [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: MongoDB::SelectionError: No writable server available. MongoDB server status: [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: Topology type: Single; Member status: [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution ) make: *** [Makefile:154: import_sample data] Error 22 Solution: The cause of this issue is that you already have the mongodb database server running on your local machine at port 27017. For linux users: First stop the MongoDB server from your OS sudo systemctl stop mongod Then check that mongod is stopped with: systemctl status mongod | grep Active Note: The output of this command should be: Active: inactive (dead) Then, executed this: docker-compose up Note: To know more about docker-compose commands do read this guide make dev error: [build_lang] Error 13 - can't write into /mnt/podata/data/Lang.openfoodfacts.localhost.sto # When running make dev : <h1>Software error:</h1> <pre>can't write into /mnt/podata/data/Lang.openfoodfacts.localhost.sto: Permission denied at /opt/product-opener/lib/ProductOpener/Store.pm line 234. </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. </p> make: *** [Makefile:126: build_lang] Error 13 Solution: Use the powershell/cmd to run the make dev commands in windows.","title":"How to setup the Dev environment (quick start guide)"},{"location":"dev/how-to-quick-start-guide/#how-to-setup-the-dev-environment-quick-start-guide","text":"This guide will allow you to rapidly build a ready-to-use development environment for Product Opener running in Docker. As an alternative to setting up your environment locally, follow the Gitpod how-to guide to instantly provision a ready-to-code development environment in the cloud. First setup time estimate is ~10min with the following specs: * 8 GB of RAM dedicated to Docker client * 6 cores dedicated to Docker client * 12 MB/s internet speed","title":"How to setup the Dev environment (quick start guide)"},{"location":"dev/how-to-quick-start-guide/#1-prerequisites","text":"Docker is the easiest way to install the Open Food Facts server, play with it, and even modify the code. Docker provides an isolated environment, very close to a Virtual Machine. This environment contains everything required to launch the Open Food Facts server. There is no need to install Perl, Perl modules, Nginx, nor Apache separately. Installation steps: - Install Docker CE If you run e.g. Debian, don't forget to add your user to the docker group! - Install Docker Compose - Enable command-line completion","title":"1. Prerequisites"},{"location":"dev/how-to-quick-start-guide/#windows-prerequisites","text":"When running with Windows, install Docker Desktop which will cover all of the above. The Make tasks use a number of Linux commands, such as rm and nproc, so it is recommeded to run Make commands from the Git Bash shell. In addition, the following need to be installed and included in the PATH: Make for Windows wget for windows (In order to download the full product database). The process of cloning the repository will create a number of symbolic links which require specific permissions under Windows. In order to do this you can use any one of these alternatives: Use an Administrative command prompt for all Git commands Completely disable UAC Specifically grant the Create symbolic links permission to your user","title":"Windows Prerequisites"},{"location":"dev/how-to-quick-start-guide/#2-clone-the-repository-from-github","text":"You must have a GitHub account if you want to contribute to Open Food Facts development, but it\u2019s not required if you just want to see how it works. Be aware Open Food Facts server takes more than 1.3 GB (2019/11). Choose your prefered way to clone, either:","title":"2. Clone the repository from GitHub"},{"location":"dev/how-to-quick-start-guide/#on-windows","text":"If you are running Docker on Windows, please use the following git clone command: git clone -c core.symlinks=true https://github.com/openfoodfacts/openfoodfacts-server.git or git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git","title":"On Windows:"},{"location":"dev/how-to-quick-start-guide/#on-other-systems","text":"git clone git@github.com:openfoodfacts/openfoodfacts-server.git or git clone https://github.com/openfoodfacts/openfoodfacts-server.git Go to the cloned directory: cd openfoodfacts-server/","title":"On other systems:"},{"location":"dev/how-to-quick-start-guide/#3-optional-review-product-openers-environment","text":"Note: you can skip this step for the first setup since the default .env in the repo contains all the default values required to get started. Before running the docker-compose deployment, you can review and configure Product Opener's environment ( .env file). The .env file contains ProductOpener default settings: | Field | Description | | ----------------------------------------------------------------- | --- | | PRODUCT_OPENER_DOMAIN | Can be set to different values based on which OFF flavor is run.| | PRODUCT_OPENER_PORT | can be modified to run NGINX on a different port. Useful when running multiple OFF flavors on different ports on the same host. Default port: 80 .| | PRODUCT_OPENER_FLAVOR | Can be modified to run different flavors of OpenFoodFacts, amongst openfoodfacts (default), openbeautyfacts , openpetfoodfacts , openproductsfacts .| | PRODUCT_OPENER_FLAVOR_SHORT | can be modified to run different flavors of OpenFoodFacts, amongst off (default), obf , oppf , opf .| | PRODUCERS_PLATFORM | can be set to 1 to build / run the producer platform .| | ROBOTOFF_URL | can be set to connect with a Robotoff instance .| | REDIS_URL | can be set to connect with a Redis instance for populating the search index .| | GOOGLE_CLOUD_VISION_API_KEY | can be set to enable OCR using Google Cloud Vision .| | CROWDIN_PROJECT_IDENTIFIER and CROWDIN_PROJECT_KEY | can be set to run translations .| | GEOLITE2_PATH , GEOLITE2_ACCOUNT_ID and GEOLITE2_LICENSE_KEY | can be set to enable Geolite2 .| | TAG | Is set to latest by default, but you can specify any Docker Hub tag for the frontend / backend images. Note that this is useful only if you use pre-built images from the Docker Hub ( docker/prod.yml override); the default dev setup ( docker/dev.yml ) builds images locally| The .env file also contains some useful Docker Compose variables: * COMPOSE_PROJECT_NAME is the compose project name that sets the prefix to every container name . Do not update this unless you know what you're doing. * COMPOSE_FILE is the ; -separated list of Docker compose files that are included in the deployment: * For a development -like environment, set it to docker-compose.yml;docker/dev.yml (default) * For a production -like environment, set it to docker-compose.yml;docker/prod.yml;docker/mongodb.yml * For more features, you can add: * docker/admin-uis.yml : add the Admin UIS container * docker/geolite2.yml : add the Geolite2 container * docker/perldb.yml : add the Perl debugger container * COMPOSE_SEPARATOR is the separator used for COMPOSE_FILE . Note: Instead of modifying .env (with the risk commit it inadvertently), You can also set needed variables in your shell, they will override .env values. Consider creating a .envrc file that you source each time you need to work on the project. On linux and macOS, you can automatically do it if you use direnv .","title":"3. [Optional] Review Product Opener's environment"},{"location":"dev/how-to-quick-start-guide/#4-build-your-dev-environment","text":"From the repository root, run: make dev Note: If you are using Windows, you may encounter issues regarding this command. Take a look at the Troubleshooting section further in this tutorial. Note: If docker complains about ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network It can be solved by adding {\"base\": \"172.80.0.0/16\",\"size\": 24}, {\"base\": \"172.90.0.0/16\",\"size\": 24} to default-address-pools in /etc/docker/daemon.json and then restarting the docker daemon. Credits to https://theorangeone.net/posts/increase-docker-ip-space/ for this solution. The command will run 2 subcommands: * make up : Build and run containers from the local directory and bind local code files, so that you do not have to rebuild everytime. * make import_sample_data : Load sample data into mongodb container (~100 products). Notes: The first build can take between 10 and 30 minutes depending on your machine and internet connection (broadband connection heavily recommended, as this will download Docker base images, install Debian and Perl modules in preparation of the final container image). You might not immediately see the test products: create an account, login, and they should appear. For a full description of available make targets, see docker/README.md Hosts file: Since the default PRODUCT_OPENER_DOMAIN in the .env file is set to openfoodfacts.localhost , add the following to your hosts file (Windows: C:\\Windows\\System32\\drivers\\etc\\hosts ; Linux/MacOSX: /etc/hosts ): 127.0.0.1 world.openfoodfacts.localhost fr.openfoodfacts.localhost static.openfoodfacts.localhost ssl-api.openfoodfacts.localhost fr-en.openfoodfacts.localhost You're done ! Check http://openfoodfacts.localhost/ !","title":"4. Build your dev environment"},{"location":"dev/how-to-quick-start-guide/#going-further","text":"To learn more about developing with Docker, see the Docker developer's guide . To have all site page on your dev instance, see Using pages from openfoodfacts-web Using Repl offers you a way to play with perl. Specific notes are provide on applying AGRIBALYSE updates to support the Ecoscore calculation.","title":"Going further"},{"location":"dev/how-to-quick-start-guide/#visual-studio-code","text":"WARNING : for now this is deprecated, some work needs to be done. This repository comes with a configuration for Visual Studio Code (VS Code) development containers (devcontainer) . This enables some Perl support in VS Code without the need to install the correct Perl version and modules on your local machine. To use the devcontainer, install prerequisites , clone the repository from GitHub , and (optionally) review Product Opener's environment . Additionally, install Visual Studio Code . VS Code will automatically recommend some extensions, but if you don't want to install all of them, please do install Remote - Containers manually. You can then use the extension command Remote-Containers: Reopen Folder in Container , which will automatically build the container and start the services. No need to use make !","title":"Visual Studio Code"},{"location":"dev/how-to-quick-start-guide/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"dev/how-to-quick-start-guide/#make-dev-error-make-command-not-found","text":"When running make dev : bash: make: command not found Solution: Click the Windows button, then type \u201cenvironment properties\u201d into the search bar and hit Enter. Click Environment Variables, then under System variables choose Path and click Edit. Click New and insert C:\\Program Files (x86)\\GnuWin32\\bin, then save the changes. Open a new terminal and test that the command works. (see Make Windows for more)","title":"make dev error: make: command not found"},{"location":"dev/how-to-quick-start-guide/#make-dev-error-build_lang-error-2-could-not-load-taxonomy-mntpodatataxonomiestracesresultsto","text":"When running make dev : <h1>Software error:</h1> <pre>Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. </p> [Tue Apr 5 19:36:40 2022] build_lang.pl: Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. [Tue Apr 5 19:36:40 2022] build_lang.pl: Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. [Tue Apr 5 19:36:40 2022] build_lang.pl: BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. make: *** [build_lang] Error 2 Solution: Project needs Symlinks to be enabled. traces.result.sto is a symlink to allergens.result.sto You have to enable the 'Developer Mode' in order to use the symlinks. To enable Developer Mode: on windows 10: Settings > Update & Security > 'For developers' \u2026 on windows 11: Settings > Privacy & Security > 'For developers' \u2026 and turn on the toggle for Developer Mode . On Windows systems, the git repository needs to be cloned with symlinks enabled. You need to remove current directory where you clone the project, and clone the project again, using right options: git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git","title":"make dev error: [build_lang] Error 2 - Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto"},{"location":"dev/how-to-quick-start-guide/#rm-is-not-recognized-as-an-internal-or-external-command","text":"When running make import_prod_data or some other commands. Solution: Use the Git Bash shell to run the make commands in windows so that programs like nproc and rm are found.","title":"'rm' is not recognized as an internal or external command"},{"location":"dev/how-to-quick-start-guide/#system-cannot-find-wget","text":"When running make import_prod_data . process_begin: CreateProcess(NULL, wget --no-verbose https://static.openfoodfacts.org/data/openfoodfacts-mongodbdump.tar.gz, ...) failed. make (e=2): The system cannot find the file specified. You need to install wget for windows . The referenced version is able to use the Windows Certificate Store, whereas the standard gnuwin32 version will give errors about not being able to verify the server certificate.","title":"System cannot find wget"},{"location":"dev/how-to-quick-start-guide/#make-makefile154-import_sample_data-error-22","text":"When running make import_sample_data <hl>Software error:</h1> <pre>MongoDB: :SelectionError: No writable server available. MongoDB server status: Topology type: Single; Member status: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution ) </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. <p> [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: MongoDB::SelectionError: No writable server available. MongoDB server status: [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: Topology type: Single; Member status: [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution ) make: *** [Makefile:154: import_sample data] Error 22 Solution: The cause of this issue is that you already have the mongodb database server running on your local machine at port 27017. For linux users: First stop the MongoDB server from your OS sudo systemctl stop mongod Then check that mongod is stopped with: systemctl status mongod | grep Active Note: The output of this command should be: Active: inactive (dead) Then, executed this: docker-compose up Note: To know more about docker-compose commands do read this guide","title":"make: *** [Makefile:154: import_sample_data] Error 22"},{"location":"dev/how-to-quick-start-guide/#make-dev-error-build_lang-error-13-cant-write-into-mntpodatadatalangopenfoodfactslocalhoststo","text":"When running make dev : <h1>Software error:</h1> <pre>can't write into /mnt/podata/data/Lang.openfoodfacts.localhost.sto: Permission denied at /opt/product-opener/lib/ProductOpener/Store.pm line 234. </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. </p> make: *** [Makefile:126: build_lang] Error 13 Solution: Use the powershell/cmd to run the make dev commands in windows.","title":"make dev error: [build_lang] Error 13 - can't write into /mnt/podata/data/Lang.openfoodfacts.localhost.sto"},{"location":"dev/how-to-update-agribalyse-ecoscore/","text":"How to update Agribalyse (Ecoscore) # Open Food Facts calculates the Ecoscore of a product from the Categories taxonomy where this has been linked to an AGRIBALYSE food code or proxy. New versions of the AGRIBALYSE database are released from time to time and this document explains how to apply updates. The high-level steps are as follows: Obtain and Convert the AGRIBALYSE Spreadsheet # Download the AGRIBALYSE food spreadsheet from the AGRIBALYSE web site (use the French site rather than English as updates on the English site may be delayed), and save it as AGRIBALYSE_vf.xlsm\" in the ecoscore/agribalyse folder. In a backend shell run the ssconvert.sh script. This will re-generate the CSV files, including the AGRIBALYSE_version and AGRIBALYSE_summary files. The AGRIBALYSE_summary file is sorted to make for easier comparison with the previous version. The Ecoscore calculation just uses the data from the \"Detail etape\" tab, which is converted to AGRIBALYSE_vf.csv.2 by ssconvert. The Ecoscore.pm module skips the first three lines of this file to ignore headers. This should be checked for each update as the number of header lines has previously changed. Also check that none of the column headings have changed. Review and fix any changed Categories # Review the changes to AGRIBALYSE_summary to determine if any codes have been removed or significantly edited and update the Categories taxonomy accordingly. Once the Categories have been updated you will need to build the taxonomies. You can then update unit test results with the update_tests_results.sh script to see if any have been affected. It is also worth checking the impact the update has had on the main product database. This can be downloaded locally and the differences determined by running the update_all_produycts script. The previous values of the Ecoscore are stored in the previous_data section under ecoscore_data. Before applying an update you will need to delete this section with the following MongoDB script: db . products . update ({}, { $unset : { \"ecoscore_data.previous_data\" : 0 }}); You can then use the following script from a backend bash shell to update products: ./update_all_products.pl --fields categories --compute-ecoscore The process will set the en:ecoscore_grade_changed and en:ecoscore_changed misc_tags, which can be queried to analyse the results. For example, the following script generates a CSV file that summaries all the categories where the grade has changed: var results = db . products . aggregate ([ { $match : { misc_tags : \"en:ecoscore-grade-changed\" } }, { $group : { _id : { en : \"$ecoscore_data.agribalyse.name_en\" , fr : \"$ecoscore_data.agribalyse.name_fr\" , code_before : \"$ecoscore_data.previous_data.agribalyse.code\" , code_after : \"$ecoscore_data.agribalyse.code\" , before : \"$ecoscore_data.previous_data.grade\" , after : \"$ecoscore_data.grade\" }, count : { $sum : 1 } } } ]). toArray (); print ( 'en.Name,fr.Name,Code Before,Code After,Grade Before,Grade After,Count' ); results . forEach (( result ) => { // eslint-disable-next-line no-underscore-dangle var id = result . _id ; print ( '\"' + ( id . en || '' ). replace ( /\"/g , '\"\"' ) + '\",\"' + ( id . fr || '' ). replace ( /\"/g , '\"\"' ) + '\",' + id . code_before + ',' + id . code_after + ',' + id . before + ',' + id . after + ',' + result . count ); }); The following script fetches the specific products that have changed: var products = db . products . find ( { misc_tags : \"en:ecoscore-grade-changed\" }, { _id : 1 , \"ecoscore_data.agribalyse.name_en\" : 1 , \"ecoscore_data.agribalyse.name_fr\" : 1 , \"ecoscore_data_main.agribalyse.code\" : 1 , \"ecoscore_data.previous_data.agribalyse.code\" : 1 , \"ecoscore_data.agribalyse.code\" : 1 , \"ecoscore_data_main.grade\" : 1 , \"ecoscore_data.previous_data.grade\" : 1 , \"ecoscore_data.grade\" : 1 , \"ecoscore_data_main.score\" : 1 , \"ecoscore_data.previous_data.score\" : 1 , \"ecoscore_data.score\" : 1 , \"ecoscore_data_main.agribalyse.ef_total\" : 1 , \"ecoscore_data.previous_data.agribalyse.ef_total\" : 1 , \"ecoscore_data.agribalyse.ef_total\" : 1 , \"categories_tags\" : 1 }). toArray (); print ( '_id,en.Name,fr.Name,Code Before Main,Code Before Change,Code After,Grade Before Main,Grade Before Change,Grade After,Score Before Main,Score Before Change,Score After,ef_total Before Main,ef_total Before Change,ef_total After,Categories Tags' ); products . forEach (( result ) => { var ecoscore_data_main = result . ecoscore_data_main || {}; var ecoscore_data_main_agribalyse = ecoscore_data_main . agribalyse || {}; // eslint-disable-next-line no-underscore-dangle print ( result . _id + ',\"' + ( result . ecoscore_data . agribalyse . name_en || '' ). replace ( /\"/g , '\"\"' ) + '\",\"' + ( result . ecoscore_data . agribalyse . name_fr || '' ). replace ( /\"/g , '\"\"' ) + '\",' + ecoscore_data_main_agribalyse . code + ',' + result . ecoscore_data . previous_data . agribalyse . code + ',' + result . ecoscore_data . agribalyse . code + ',' + ecoscore_data_main . grade + ',' + result . ecoscore_data . previous_data . grade + ',' + result . ecoscore_data . grade + ',' + ecoscore_data_main . score + ',' + result . ecoscore_data . previous_data . score + ',' + result . ecoscore_data . score + ',' + ecoscore_data_main_agribalyse . ef_total + ',' + result . ecoscore_data . previous_data . agribalyse . ef_total + ',' + result . ecoscore_data . agribalyse . ef_total + ',\"' + result . categories_tags . join ( \" \" ) + '\"' ); }); Link existing Categories to new AGRIBALYSE codes # If a new AGRIBALYSE category matches and existing OFF Category then the two can be linked by adding an agribalyse_food_code:en tag. If there is not a precise match then add an agribalyse_proxy_food_code:en tag along with the agribalyse_proxy_food_name:en and agribalyse_proxy_food_name:fr tags. Re-run the update_all_products script after doing this to assess how many products now have an Ecoscore when they did not previously. Use the above scripts to analyse the MongoDB, the new categories will have previous values of undefined . Add new Categories for new AGRIBALYSE codes # For any new categories, review the AGRIBALYSE category descriptions to ensure they are concise and unambiguous sucgh that an OFF user is most likely to get a match on a type-ahead search. Give notice of the change on the taxonomies channel in Slack so that additional translations can be added for the new categories. It is not necessary to add a category for every single AGRIBALYSE entry. For example, AGRIBALYSE has over 80 codes for different mineral waters but these all have almost exactly the same environmental impact. In cases like this it is acceptable to pick a single representative AGRIBALYSE code as a proxy for the Category in general. It may be worth doing a final check to see how many categories cominations still do not have a match to AGRIBALYSE: var missing = db . products . aggregate ([ { $match : { \"ecoscore_data.grade\" : null } }, { $group : { _id : \"$categories_tags\" , count : { $sum : 1 } } } ]). toArray (); print ( 'Category,Count' ); missing . forEach (( result ) => { // eslint-disable-next-line no-underscore-dangle var id = result . _id ; print ( '\"' + ( id . join ( ',' ) || '' ). replace ( /\"/g , '\"\"' ) + '\",' + result . count ); });","title":"How to update Agribalyse (Ecoscore)"},{"location":"dev/how-to-update-agribalyse-ecoscore/#how-to-update-agribalyse-ecoscore","text":"Open Food Facts calculates the Ecoscore of a product from the Categories taxonomy where this has been linked to an AGRIBALYSE food code or proxy. New versions of the AGRIBALYSE database are released from time to time and this document explains how to apply updates. The high-level steps are as follows:","title":"How to update Agribalyse (Ecoscore)"},{"location":"dev/how-to-update-agribalyse-ecoscore/#obtain-and-convert-the-agribalyse-spreadsheet","text":"Download the AGRIBALYSE food spreadsheet from the AGRIBALYSE web site (use the French site rather than English as updates on the English site may be delayed), and save it as AGRIBALYSE_vf.xlsm\" in the ecoscore/agribalyse folder. In a backend shell run the ssconvert.sh script. This will re-generate the CSV files, including the AGRIBALYSE_version and AGRIBALYSE_summary files. The AGRIBALYSE_summary file is sorted to make for easier comparison with the previous version. The Ecoscore calculation just uses the data from the \"Detail etape\" tab, which is converted to AGRIBALYSE_vf.csv.2 by ssconvert. The Ecoscore.pm module skips the first three lines of this file to ignore headers. This should be checked for each update as the number of header lines has previously changed. Also check that none of the column headings have changed.","title":"Obtain and Convert the AGRIBALYSE Spreadsheet"},{"location":"dev/how-to-update-agribalyse-ecoscore/#review-and-fix-any-changed-categories","text":"Review the changes to AGRIBALYSE_summary to determine if any codes have been removed or significantly edited and update the Categories taxonomy accordingly. Once the Categories have been updated you will need to build the taxonomies. You can then update unit test results with the update_tests_results.sh script to see if any have been affected. It is also worth checking the impact the update has had on the main product database. This can be downloaded locally and the differences determined by running the update_all_produycts script. The previous values of the Ecoscore are stored in the previous_data section under ecoscore_data. Before applying an update you will need to delete this section with the following MongoDB script: db . products . update ({}, { $unset : { \"ecoscore_data.previous_data\" : 0 }}); You can then use the following script from a backend bash shell to update products: ./update_all_products.pl --fields categories --compute-ecoscore The process will set the en:ecoscore_grade_changed and en:ecoscore_changed misc_tags, which can be queried to analyse the results. For example, the following script generates a CSV file that summaries all the categories where the grade has changed: var results = db . products . aggregate ([ { $match : { misc_tags : \"en:ecoscore-grade-changed\" } }, { $group : { _id : { en : \"$ecoscore_data.agribalyse.name_en\" , fr : \"$ecoscore_data.agribalyse.name_fr\" , code_before : \"$ecoscore_data.previous_data.agribalyse.code\" , code_after : \"$ecoscore_data.agribalyse.code\" , before : \"$ecoscore_data.previous_data.grade\" , after : \"$ecoscore_data.grade\" }, count : { $sum : 1 } } } ]). toArray (); print ( 'en.Name,fr.Name,Code Before,Code After,Grade Before,Grade After,Count' ); results . forEach (( result ) => { // eslint-disable-next-line no-underscore-dangle var id = result . _id ; print ( '\"' + ( id . en || '' ). replace ( /\"/g , '\"\"' ) + '\",\"' + ( id . fr || '' ). replace ( /\"/g , '\"\"' ) + '\",' + id . code_before + ',' + id . code_after + ',' + id . before + ',' + id . after + ',' + result . count ); }); The following script fetches the specific products that have changed: var products = db . products . find ( { misc_tags : \"en:ecoscore-grade-changed\" }, { _id : 1 , \"ecoscore_data.agribalyse.name_en\" : 1 , \"ecoscore_data.agribalyse.name_fr\" : 1 , \"ecoscore_data_main.agribalyse.code\" : 1 , \"ecoscore_data.previous_data.agribalyse.code\" : 1 , \"ecoscore_data.agribalyse.code\" : 1 , \"ecoscore_data_main.grade\" : 1 , \"ecoscore_data.previous_data.grade\" : 1 , \"ecoscore_data.grade\" : 1 , \"ecoscore_data_main.score\" : 1 , \"ecoscore_data.previous_data.score\" : 1 , \"ecoscore_data.score\" : 1 , \"ecoscore_data_main.agribalyse.ef_total\" : 1 , \"ecoscore_data.previous_data.agribalyse.ef_total\" : 1 , \"ecoscore_data.agribalyse.ef_total\" : 1 , \"categories_tags\" : 1 }). toArray (); print ( '_id,en.Name,fr.Name,Code Before Main,Code Before Change,Code After,Grade Before Main,Grade Before Change,Grade After,Score Before Main,Score Before Change,Score After,ef_total Before Main,ef_total Before Change,ef_total After,Categories Tags' ); products . forEach (( result ) => { var ecoscore_data_main = result . ecoscore_data_main || {}; var ecoscore_data_main_agribalyse = ecoscore_data_main . agribalyse || {}; // eslint-disable-next-line no-underscore-dangle print ( result . _id + ',\"' + ( result . ecoscore_data . agribalyse . name_en || '' ). replace ( /\"/g , '\"\"' ) + '\",\"' + ( result . ecoscore_data . agribalyse . name_fr || '' ). replace ( /\"/g , '\"\"' ) + '\",' + ecoscore_data_main_agribalyse . code + ',' + result . ecoscore_data . previous_data . agribalyse . code + ',' + result . ecoscore_data . agribalyse . code + ',' + ecoscore_data_main . grade + ',' + result . ecoscore_data . previous_data . grade + ',' + result . ecoscore_data . grade + ',' + ecoscore_data_main . score + ',' + result . ecoscore_data . previous_data . score + ',' + result . ecoscore_data . score + ',' + ecoscore_data_main_agribalyse . ef_total + ',' + result . ecoscore_data . previous_data . agribalyse . ef_total + ',' + result . ecoscore_data . agribalyse . ef_total + ',\"' + result . categories_tags . join ( \" \" ) + '\"' ); });","title":"Review and fix any changed Categories"},{"location":"dev/how-to-update-agribalyse-ecoscore/#link-existing-categories-to-new-agribalyse-codes","text":"If a new AGRIBALYSE category matches and existing OFF Category then the two can be linked by adding an agribalyse_food_code:en tag. If there is not a precise match then add an agribalyse_proxy_food_code:en tag along with the agribalyse_proxy_food_name:en and agribalyse_proxy_food_name:fr tags. Re-run the update_all_products script after doing this to assess how many products now have an Ecoscore when they did not previously. Use the above scripts to analyse the MongoDB, the new categories will have previous values of undefined .","title":"Link existing Categories to new AGRIBALYSE codes"},{"location":"dev/how-to-update-agribalyse-ecoscore/#add-new-categories-for-new-agribalyse-codes","text":"For any new categories, review the AGRIBALYSE category descriptions to ensure they are concise and unambiguous sucgh that an OFF user is most likely to get a match on a type-ahead search. Give notice of the change on the taxonomies channel in Slack so that additional translations can be added for the new categories. It is not necessary to add a category for every single AGRIBALYSE entry. For example, AGRIBALYSE has over 80 codes for different mineral waters but these all have almost exactly the same environmental impact. In cases like this it is acceptable to pick a single representative AGRIBALYSE code as a proxy for the Category in general. It may be worth doing a final check to see how many categories cominations still do not have a match to AGRIBALYSE: var missing = db . products . aggregate ([ { $match : { \"ecoscore_data.grade\" : null } }, { $group : { _id : \"$categories_tags\" , count : { $sum : 1 } } } ]). toArray (); print ( 'Category,Count' ); missing . forEach (( result ) => { // eslint-disable-next-line no-underscore-dangle var id = result . _id ; print ( '\"' + ( id . join ( ',' ) || '' ). replace ( /\"/g , '\"\"' ) + '\",' + result . count ); });","title":"Add new Categories for new AGRIBALYSE codes"},{"location":"dev/how-to-use-direnv/","text":"How to use direnv # As a developer, it can be better not to think too much about setting right env variables as you enter a project. direnv aims at providing a solution. As a quick guide as an openfoodfacts developer: install direnv on your system using usual package manager in your .bashrc add: # direnv eval \" $( direnv hook bash ) \" you have adapt the direnv line according to what you use, see direnv doc In your project directory add a file, where you superseed variables from .env that you wan't to echo \"setting up docker-compose env\" export DOCKER_BUILDKIT=1 export USER_UID=${UID} export USER_UID=$(id -g) in project directory, run direnv allow . in a new shell: go in project directory you should have direnv trigger and load your variables","title":"How to use direnv"},{"location":"dev/how-to-use-direnv/#how-to-use-direnv","text":"As a developer, it can be better not to think too much about setting right env variables as you enter a project. direnv aims at providing a solution. As a quick guide as an openfoodfacts developer: install direnv on your system using usual package manager in your .bashrc add: # direnv eval \" $( direnv hook bash ) \" you have adapt the direnv line according to what you use, see direnv doc In your project directory add a file, where you superseed variables from .env that you wan't to echo \"setting up docker-compose env\" export DOCKER_BUILDKIT=1 export USER_UID=${UID} export USER_UID=$(id -g) in project directory, run direnv allow . in a new shell: go in project directory you should have direnv trigger and load your variables","title":"How to use direnv"},{"location":"dev/how-to-use-gitpod/","text":"How to using Gitpod for Remote Development # If your computer performance restricts you from developing, you can use Gitpod . Gitpod allows you to do the devs on an ephemeral environment. It is free for a maximum of 500 credits or 50 hours per months (https://www.gitpod.io/pricing). Gitpod provides a robust ready-to-code developer environment in the cloud eliminating the friction of setting up local environments and IDEs with Perl, Docker and plugins, making it possible for even new contributors to OpenFoodFacts Server to get started in minutes instead of hours! Note that while this how-to is tailored for Gitpod, using alternatives like GitHub Codespaces should be similar. For the most part, development on Gitpod is similar to developing locally as documented in the quickstart guide and docker-developer-guide , however accessing your dev-deployment of openfoodfacts-server requires an extra step. Connect GitHub and Gitpod # When you use Gitpod, you allow Gitpod to use your GitHub account. In GitHub, you can review (and revoke if you stop using Gitpod) the access granted to Gitpod: click on your avatar on top right of the screen, then, Settings. In the left panel, under Integrations, click on Applications, then, Authorized OAuth Apps. On the Gitpod side, you can also update what Gitpod is allowed to do with your GitHub account: click on your avatar on the top right of the screen, then, Settings. In the left panel, click on Integrations. The line for GitHub should be green. At the end of this line, click on the three dots, then Edit Permissions. '''If you want to create a pull request via Gitpod, you need to grant public_repo access.''' Get Started # Gitpod will automatically clone and open the repository for you in VSCode by default. It will also automatically build the project for you on opening and comes with Docker and other tools pre-installed making it one of the fastest ways to spin up an environment for openfoodfacts-server . Once the repository is open in Gitpod, other instructions in the quick-start guide can be generally followed. Accessing your development instance of OpenFoodFacts Web # Since Gitpod runs your code in a remote machine, your dev-deployment spun up with make dev or make up will not accessible when you open the default http://openfoodfacts.localhost in your browser. This occurs because the server running on the remote machine is not accessible on your local network interface. To overcome this, we can make use of SSH tunnel that listens to your local port 80 and forwards traffic to the port 80 of the remote machine. Gitpod makes it really simple to SSH into your dev environment by letting you copy the ssh command required to reach your remote environment. To start, follow the ssh instructions on Gitpod's official guide: SSH for workspaces as easy as copy/paste . Once you have copied the ssh command and ensure it works as-is, add a -L 80:localhost:80 to the command to make it look like: ssh -L 80:localhost:80 'openfoodfac-openfoodfac-tok-openfoodfac-r9f61214h9vt.ssh.ws-c.gitpod.io' . Once you execute the altered command in your terminal, you should be able to access OpenFoodFacts on http://openfoodfacts.localhost just as documented in the quickstart guide! Remark: for some Linux distributions, the port 80 is reserved. A workaround is to switch to port 8080: in gitpod, open the .env file and replace the line PRODUCT_OPENER_PORT=80 by PRODUCT_OPENER_PORT=8080, then replace -L 80:localhost:80 by -L 8080:localhost:8080. Rollback the changes on .env before to make a pull request! * Remark: the address to connect with ssh can change after few days. If you get a Connection closed by ... port 22 simply go back to https://gitpod.io/workspaces and copy the new address. Remark: if you load the page after some changes but get a 502 Bad Gateway check again your code. Something may be wrong with it. Eventually, try to comment the part you just coded to see if it works. Create an account to be able to edit products. Some commands # After you made devs and want to apply changes and see them on the website, you can run: $ docker-compose restart $ make up If you face some difficulties, you can always look at the logs (use ctrl + c, to quit): $ make log $ make tail After development, before opening a pull request, run the following command: $ make checks","title":"How to using Gitpod for Remote Development"},{"location":"dev/how-to-use-gitpod/#how-to-using-gitpod-for-remote-development","text":"If your computer performance restricts you from developing, you can use Gitpod . Gitpod allows you to do the devs on an ephemeral environment. It is free for a maximum of 500 credits or 50 hours per months (https://www.gitpod.io/pricing). Gitpod provides a robust ready-to-code developer environment in the cloud eliminating the friction of setting up local environments and IDEs with Perl, Docker and plugins, making it possible for even new contributors to OpenFoodFacts Server to get started in minutes instead of hours! Note that while this how-to is tailored for Gitpod, using alternatives like GitHub Codespaces should be similar. For the most part, development on Gitpod is similar to developing locally as documented in the quickstart guide and docker-developer-guide , however accessing your dev-deployment of openfoodfacts-server requires an extra step.","title":"How to using Gitpod for Remote Development"},{"location":"dev/how-to-use-gitpod/#connect-github-and-gitpod","text":"When you use Gitpod, you allow Gitpod to use your GitHub account. In GitHub, you can review (and revoke if you stop using Gitpod) the access granted to Gitpod: click on your avatar on top right of the screen, then, Settings. In the left panel, under Integrations, click on Applications, then, Authorized OAuth Apps. On the Gitpod side, you can also update what Gitpod is allowed to do with your GitHub account: click on your avatar on the top right of the screen, then, Settings. In the left panel, click on Integrations. The line for GitHub should be green. At the end of this line, click on the three dots, then Edit Permissions. '''If you want to create a pull request via Gitpod, you need to grant public_repo access.'''","title":"Connect GitHub and Gitpod"},{"location":"dev/how-to-use-gitpod/#get-started","text":"Gitpod will automatically clone and open the repository for you in VSCode by default. It will also automatically build the project for you on opening and comes with Docker and other tools pre-installed making it one of the fastest ways to spin up an environment for openfoodfacts-server . Once the repository is open in Gitpod, other instructions in the quick-start guide can be generally followed.","title":"Get Started"},{"location":"dev/how-to-use-gitpod/#accessing-your-development-instance-of-openfoodfacts-web","text":"Since Gitpod runs your code in a remote machine, your dev-deployment spun up with make dev or make up will not accessible when you open the default http://openfoodfacts.localhost in your browser. This occurs because the server running on the remote machine is not accessible on your local network interface. To overcome this, we can make use of SSH tunnel that listens to your local port 80 and forwards traffic to the port 80 of the remote machine. Gitpod makes it really simple to SSH into your dev environment by letting you copy the ssh command required to reach your remote environment. To start, follow the ssh instructions on Gitpod's official guide: SSH for workspaces as easy as copy/paste . Once you have copied the ssh command and ensure it works as-is, add a -L 80:localhost:80 to the command to make it look like: ssh -L 80:localhost:80 'openfoodfac-openfoodfac-tok-openfoodfac-r9f61214h9vt.ssh.ws-c.gitpod.io' . Once you execute the altered command in your terminal, you should be able to access OpenFoodFacts on http://openfoodfacts.localhost just as documented in the quickstart guide! Remark: for some Linux distributions, the port 80 is reserved. A workaround is to switch to port 8080: in gitpod, open the .env file and replace the line PRODUCT_OPENER_PORT=80 by PRODUCT_OPENER_PORT=8080, then replace -L 80:localhost:80 by -L 8080:localhost:8080. Rollback the changes on .env before to make a pull request! * Remark: the address to connect with ssh can change after few days. If you get a Connection closed by ... port 22 simply go back to https://gitpod.io/workspaces and copy the new address. Remark: if you load the page after some changes but get a 502 Bad Gateway check again your code. Something may be wrong with it. Eventually, try to comment the part you just coded to see if it works. Create an account to be able to edit products.","title":"Accessing your development instance of OpenFoodFacts Web"},{"location":"dev/how-to-use-gitpod/#some-commands","text":"After you made devs and want to apply changes and see them on the website, you can run: $ docker-compose restart $ make up If you face some difficulties, you can always look at the logs (use ctrl + c, to quit): $ make log $ make tail After development, before opening a pull request, run the following command: $ make checks","title":"Some commands"},{"location":"dev/how-to-use-pages-from-openfoodfacts-web/","text":"How to use pages from openfoodfacts-web # To avoid messing product-opener repository with translations of web-pages, we moved most pages in openfoodfacts-web repository specificly in the lang/ directory. This repo only has a really minimal lang directory named lang-default. If you want to have all contents locally, you should first clone openfoodfacts-web repo locally, and then: if you are using docker, you can set the WEB_LANG_PATH env variable to a relative or absolute path leading to openfoodfacts-web lang directory. else, make symlink lang point to openfoodfacts-web lang directory.","title":"How to use pages from openfoodfacts-web"},{"location":"dev/how-to-use-pages-from-openfoodfacts-web/#how-to-use-pages-from-openfoodfacts-web","text":"To avoid messing product-opener repository with translations of web-pages, we moved most pages in openfoodfacts-web repository specificly in the lang/ directory. This repo only has a really minimal lang directory named lang-default. If you want to have all contents locally, you should first clone openfoodfacts-web repo locally, and then: if you are using docker, you can set the WEB_LANG_PATH env variable to a relative or absolute path leading to openfoodfacts-web lang directory. else, make symlink lang point to openfoodfacts-web lang directory.","title":"How to use pages from openfoodfacts-web"},{"location":"dev/how-to-use-repl/","text":"How to use Perl REPL (re.pl) # On your local dev instance, the \"backend\" container comes with Devel::REPL installed. This is a handy package to try out perl expressions and learn. Thanks to PERL5LIB variable which is already configured, you can load any module of ProductOpener from within it. Also it as the right Launch Repl # Just run docker-compose run --rm docker-compose re.pl If you want to access external services (like mongodb), do not forget to start them. Testing perl code # It can be a handy way to get your hand into perl by testing some code patterns, or seeing how they react. For example one can test a regular expression: $ my $text = \"Hello World\" ; Hello World $ $ text =~ /Hello (\\w+)/i World Reading a sto # Another use case is reading a sto file to see what it contains. Eg. for a user: $ use ProductOpener:: Store qw/:all/ ; $ my $user_id = \"xxxx\" ; $ my $user_ref = retrieve ( \"/mnt/podata/users/$user_id.sto\" );","title":"How to use Perl REPL (re.pl)"},{"location":"dev/how-to-use-repl/#how-to-use-perl-repl-repl","text":"On your local dev instance, the \"backend\" container comes with Devel::REPL installed. This is a handy package to try out perl expressions and learn. Thanks to PERL5LIB variable which is already configured, you can load any module of ProductOpener from within it. Also it as the right","title":"How to use Perl REPL (re.pl)"},{"location":"dev/how-to-use-repl/#launch-repl","text":"Just run docker-compose run --rm docker-compose re.pl If you want to access external services (like mongodb), do not forget to start them.","title":"Launch Repl"},{"location":"dev/how-to-use-repl/#testing-perl-code","text":"It can be a handy way to get your hand into perl by testing some code patterns, or seeing how they react. For example one can test a regular expression: $ my $text = \"Hello World\" ; Hello World $ $ text =~ /Hello (\\w+)/i World","title":"Testing perl code"},{"location":"dev/how-to-use-repl/#reading-a-sto","text":"Another use case is reading a sto file to see what it contains. Eg. for a user: $ use ProductOpener:: Store qw/:all/ ; $ my $user_id = \"xxxx\" ; $ my $user_ref = retrieve ( \"/mnt/podata/users/$user_id.sto\" );","title":"Reading a sto"},{"location":"dev/how-to-use-vscode/","text":"How to use VSCode # VSCode (or better the open source version VSCodium ) may be used to edit files. Here are some useful tricks. Perlcritic # One way to have perlcritic work is the following: install the perlcritic extension add a perlcritic.sh at the root of your project with following content: #!/usr/bin/env bash . .envrc >/dev/null 2 > & 1 docker-compose run --rm --no-deps backend perlcritic \" $@ \" 2 >/dev/null the second line is useful only if you use direnv chmod +x perlcritic.sh patch perlcritic by editing its files, following sfodje/perlcritic issue #26 the edit perlcritic configuration in workspace to set those values: Executable: /home/alex/docker/off-server/perlcritic.sh Perl Language Server # The extension Language Server and Debugger is less easy to work with ! Note: This setup does not work yet, but might not be so far. It is probably due to https://github.com/richterger/Perl-LanguageServer/issues/131 install the extension add a script shell-into-appserver.sh in the project: #!/usr/bin/env bash declare -x PATH = $PATH :/usr/local/bin/ source .envrc COMMAND = $( echo \" $@ \" | sed 's/^.*perl /perl /' ) > & 2 echo \"launching $COMMAND \" docker-compose run --rm --no-deps -T -p 127 .0.0. 1 :13603:13603 backend $COMMAND Note: the second line is useful only if you use direnv chmod +x shell-into-appserver.sh Edit workspace settings to have those settings: \"perl\" : { \"enable\" : true , \"perlInc\" : [ \"/opt/product-opener/lib\" , \"/opt/perl/local/lib/perl5\" ], \"ignoreDirs\" : [ \"/opt/perl/local/lib/perl5\" , \". vscode\" ], \"fileFilter\" : [ \".pm\" , \".pl\" , \".t\" ], \"sshAddr\" : \"dummy\" , \"sshUser\" : \"dummy\" , \"sshCmd\" : \"./shell-into-appserver.sh\" , \"sshWorkspaceRoot\" : \"/opt/product-opener\" , \"logLevel\" : 2 }, Remote container ? # Note: at the moment we do not support the Remote Container extension. While we can consider using it, it has some drawback because not all the project is contained within the \"backend\" container. For example all that concern nodejs is in the \"frontend\" container. So it means making a quite complete Docker image on its own with all the tooling necessary.","title":"How to use VSCode"},{"location":"dev/how-to-use-vscode/#how-to-use-vscode","text":"VSCode (or better the open source version VSCodium ) may be used to edit files. Here are some useful tricks.","title":"How to use VSCode"},{"location":"dev/how-to-use-vscode/#perlcritic","text":"One way to have perlcritic work is the following: install the perlcritic extension add a perlcritic.sh at the root of your project with following content: #!/usr/bin/env bash . .envrc >/dev/null 2 > & 1 docker-compose run --rm --no-deps backend perlcritic \" $@ \" 2 >/dev/null the second line is useful only if you use direnv chmod +x perlcritic.sh patch perlcritic by editing its files, following sfodje/perlcritic issue #26 the edit perlcritic configuration in workspace to set those values: Executable: /home/alex/docker/off-server/perlcritic.sh","title":"Perlcritic"},{"location":"dev/how-to-use-vscode/#perl-language-server","text":"The extension Language Server and Debugger is less easy to work with ! Note: This setup does not work yet, but might not be so far. It is probably due to https://github.com/richterger/Perl-LanguageServer/issues/131 install the extension add a script shell-into-appserver.sh in the project: #!/usr/bin/env bash declare -x PATH = $PATH :/usr/local/bin/ source .envrc COMMAND = $( echo \" $@ \" | sed 's/^.*perl /perl /' ) > & 2 echo \"launching $COMMAND \" docker-compose run --rm --no-deps -T -p 127 .0.0. 1 :13603:13603 backend $COMMAND Note: the second line is useful only if you use direnv chmod +x shell-into-appserver.sh Edit workspace settings to have those settings: \"perl\" : { \"enable\" : true , \"perlInc\" : [ \"/opt/product-opener/lib\" , \"/opt/perl/local/lib/perl5\" ], \"ignoreDirs\" : [ \"/opt/perl/local/lib/perl5\" , \". vscode\" ], \"fileFilter\" : [ \".pm\" , \".pl\" , \".t\" ], \"sshAddr\" : \"dummy\" , \"sshUser\" : \"dummy\" , \"sshCmd\" : \"./shell-into-appserver.sh\" , \"sshWorkspaceRoot\" : \"/opt/product-opener\" , \"logLevel\" : 2 },","title":"Perl Language Server"},{"location":"dev/how-to-use-vscode/#remote-container","text":"Note: at the moment we do not support the Remote Container extension. While we can consider using it, it has some drawback because not all the project is contained within the \"backend\" container. For example all that concern nodejs is in the \"frontend\" container. So it means making a quite complete Docker image on its own with all the tooling necessary.","title":"Remote container ?"},{"location":"dev/how-to-write-and-run-tests/","text":"How to write and run tests # If you are a developer you are really encouraged to write tests as you fix bug or develop new features. Having a test is also a good way to debug a particular piece of code. We would really love to see our test coverage augment. If you are new to tests, please read: - something about test pyramid to understand importance of unit tests and integration tests - perldoc on test - Test::More module doc Helpers # We have some helpers for tests. See mainly: * Test.pm (notably init_expected_results and compare_to_expected_results ) * TestAPI.pm and other modules with Test in their name ! Unit and Integration tests # Unit tests are located in tests/unit/ . Integration tests are in tests/integration/ . Most integration tests issue queries to an open food facts Integration with docker-compose # Using Makefile targets, tests are run * with a specific `COMPOSE_PROJECT_NAME\u00b0 to avoid crashing your development data while running tests (as the project name changes container, network and volumes names ) * with a specific expose port for Mongodb, to avoid clashes with dev instance. Writing tests # You can read other tests to understand how we write them (inspire yourself from recently created tests). One effective way is to create a list of tests each represented by a hashmap with inputs and expected outputs and run them in a loop. Add an id and/or a desc (description) and use it as last argument to check functions (like ok , is , \u2026) to easily see tests running and identify failing tests. If your outputs are consequent, you might use json files (one per tests), stored on disk. See tests using init_expected_results (and see below to refresh those files). Running tests # The best way to run all test is to run: make tests To run a single test you can use: for unit test: make unit-test test = \"filename.t\" for integration test: make int-test test = \"filename.t\" If you made change that impact stored expected results, you can use: to re-generate all expected results: make update_tests_results or to generate expected results for a single test (here for integration test, unit-test otherwise) make int-test = \"filename.t --update-expected results\" If you re-generate test results, be sure to look carefully that the changes your commit are expected changes. Debugging with tests # Launching a test is a very effective way to understand what's going on in the code using the debugger. This is done calling the test with perl -d . You can also use \"args\" argument with make target: make test-unit test = \"my-test.t\" args = \"-d\" Most of the time, you will have to use the next command \"n\" four times, before landing in you test, where you can easily set a breakpoint with b <line-number> . Read perldoc about debugger to learn. more. :pencil: Note: With this explanation, in integration tests that issue requests to the server, you won't be able to run the debugger inside the server code, only in the test.","title":"How to write and run tests"},{"location":"dev/how-to-write-and-run-tests/#how-to-write-and-run-tests","text":"If you are a developer you are really encouraged to write tests as you fix bug or develop new features. Having a test is also a good way to debug a particular piece of code. We would really love to see our test coverage augment. If you are new to tests, please read: - something about test pyramid to understand importance of unit tests and integration tests - perldoc on test - Test::More module doc","title":"How to write and run tests"},{"location":"dev/how-to-write-and-run-tests/#helpers","text":"We have some helpers for tests. See mainly: * Test.pm (notably init_expected_results and compare_to_expected_results ) * TestAPI.pm and other modules with Test in their name !","title":"Helpers"},{"location":"dev/how-to-write-and-run-tests/#unit-and-integration-tests","text":"Unit tests are located in tests/unit/ . Integration tests are in tests/integration/ . Most integration tests issue queries to an open food facts","title":"Unit and Integration tests"},{"location":"dev/how-to-write-and-run-tests/#integration-with-docker-compose","text":"Using Makefile targets, tests are run * with a specific `COMPOSE_PROJECT_NAME\u00b0 to avoid crashing your development data while running tests (as the project name changes container, network and volumes names ) * with a specific expose port for Mongodb, to avoid clashes with dev instance.","title":"Integration with docker-compose"},{"location":"dev/how-to-write-and-run-tests/#writing-tests","text":"You can read other tests to understand how we write them (inspire yourself from recently created tests). One effective way is to create a list of tests each represented by a hashmap with inputs and expected outputs and run them in a loop. Add an id and/or a desc (description) and use it as last argument to check functions (like ok , is , \u2026) to easily see tests running and identify failing tests. If your outputs are consequent, you might use json files (one per tests), stored on disk. See tests using init_expected_results (and see below to refresh those files).","title":"Writing tests"},{"location":"dev/how-to-write-and-run-tests/#running-tests","text":"The best way to run all test is to run: make tests To run a single test you can use: for unit test: make unit-test test = \"filename.t\" for integration test: make int-test test = \"filename.t\" If you made change that impact stored expected results, you can use: to re-generate all expected results: make update_tests_results or to generate expected results for a single test (here for integration test, unit-test otherwise) make int-test = \"filename.t --update-expected results\" If you re-generate test results, be sure to look carefully that the changes your commit are expected changes.","title":"Running tests"},{"location":"dev/how-to-write-and-run-tests/#debugging-with-tests","text":"Launching a test is a very effective way to understand what's going on in the code using the debugger. This is done calling the test with perl -d . You can also use \"args\" argument with make target: make test-unit test = \"my-test.t\" args = \"-d\" Most of the time, you will have to use the next command \"n\" four times, before landing in you test, where you can easily set a breakpoint with b <line-number> . Read perldoc about debugger to learn. more. :pencil: Note: With this explanation, in integration tests that issue requests to the server, you won't be able to run the debugger inside the server code, only in the test.","title":"Debugging with tests"},{"location":"dev/ref-docker-commands/","text":"Reference Docker / Makefile commands # See also Docker best practice at Open Food Facts The docker/ directory contains docker-compose overrides for running Product Opener on Docker . The main docker-compose file docker-compose.yml is located in the root of the repository. The step-by-step guide to setup the Product Opener using Docker is available on dev environment quick start guide . Makefile targets # Makefile targets are handy for beginners to start the project and for some usual tasks. It's better though, as you progress, if you understand how things work and be able to use targeted docker-compose commands. See also targets to run tests Command Description Notes make dev Setup a fresh dev environment. Run only once, then use the up , down , restart commands. make up Start containers. make down Stop containers and keep the volumes. Products and users data will be kept. make hdown Stop containers and delete the volumes (hard down). Products and users data will be lost ! make restart Restart frontend and backend containers. make reset Run hdown and up . make status Get containers status (up, down, fail). make log Get logs. Include only logs written to container's stdout . make tail Get other logs ( Apache , mod_perl , ...) bound to the local logs/ directory. make prune Save space by removing unused Docker artifacts. Next build will take time (no cache) ! make prune_cache Remove Docker build cache. Next build will take time (no build cache) ! make clean Clean up your dev environment: removes locally bound folders, run hdown and prune . Run make dev to recreate a fresh dev env afterwards. make import_sample_data Load sample data (~100 products) into the MongoDB database. make import_prod_data Load latest prod data (~2M products, 1.7GB) into the MongoDB database. Takes up to 10m. Not recommended for dev setups !","title":"Reference Docker / Makefile commands"},{"location":"dev/ref-docker-commands/#reference-docker-makefile-commands","text":"See also Docker best practice at Open Food Facts The docker/ directory contains docker-compose overrides for running Product Opener on Docker . The main docker-compose file docker-compose.yml is located in the root of the repository. The step-by-step guide to setup the Product Opener using Docker is available on dev environment quick start guide .","title":"Reference Docker / Makefile commands"},{"location":"dev/ref-docker-commands/#makefile-targets","text":"Makefile targets are handy for beginners to start the project and for some usual tasks. It's better though, as you progress, if you understand how things work and be able to use targeted docker-compose commands. See also targets to run tests Command Description Notes make dev Setup a fresh dev environment. Run only once, then use the up , down , restart commands. make up Start containers. make down Stop containers and keep the volumes. Products and users data will be kept. make hdown Stop containers and delete the volumes (hard down). Products and users data will be lost ! make restart Restart frontend and backend containers. make reset Run hdown and up . make status Get containers status (up, down, fail). make log Get logs. Include only logs written to container's stdout . make tail Get other logs ( Apache , mod_perl , ...) bound to the local logs/ directory. make prune Save space by removing unused Docker artifacts. Next build will take time (no cache) ! make prune_cache Remove Docker build cache. Next build will take time (no build cache) ! make clean Clean up your dev environment: removes locally bound folders, run hdown and prune . Run make dev to recreate a fresh dev env afterwards. make import_sample_data Load sample data (~100 products) into the MongoDB database. make import_prod_data Load latest prod data (~2M products, 1.7GB) into the MongoDB database. Takes up to 10m. Not recommended for dev setups !","title":"Makefile targets"},{"location":"dev/ref-perl-pod/","text":"Perl reference documentation # Do not write anything here, it is meant to be overwritten by html generated by perl pod.","title":"Perl reference documentation"},{"location":"dev/ref-perl-pod/#perl-reference-documentation","text":"Do not write anything here, it is meant to be overwritten by html generated by perl pod.","title":"Perl reference documentation"},{"location":"dev/ref-perl/","text":"Reference Perl code documentation # The documentation in Plain Old Format (aka POD) for perl module is compiled from in file documentation. See the Perl reference documentation","title":"Reference Perl code documentation"},{"location":"dev/ref-perl/#reference-perl-code-documentation","text":"The documentation in Plain Old Format (aka POD) for perl module is compiled from in file documentation. See the Perl reference documentation","title":"Reference Perl code documentation"}]}